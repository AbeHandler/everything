{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A researcher wants a generative model of entities and their attributes. The data takes the form of $\\mathcal{D}_{\\lambda} = [(e_1,a_1),(e_2,a_2) ... (e_N,a_N)] $, where $\\mathcal{\\lambda}$ is an extraction strategy, $\\mathcal{A}$ is an attribute set and $\\mathcal{E}$ is an entity set. Note that $\\lambda$ defines $\\mathcal{D}_{\\lambda}$.\n",
    "\n",
    "There are lots of reasons to prefer a generative model of $\\mathcal{D}_{\\lambda}$\n",
    "1. testing hypothesis about $(e,a)$ pairs with likelihood ratios \n",
    "2. quantifying uncertainty about conclusions with credible intervals\n",
    "3. transparently incorporating prior beliefs about entities and their attributes \n",
    "     - possibly even including prior beliefs about extraction strategies $\\lambda$, entity sets $\\mathcal{E}$ and attribute sets $\\mathcal{A}$\n",
    "4. updating priors in the face of evidence\n",
    "5. suggesting new interpretations of the data (e.g. suggesting new $a$ for inclusion in $\\mathcal{A}$, or new $e$ for inclusion in $\\mathcal{E}$)\n",
    "\n",
    "A researcher expresses their beliefs about $\\mathcal{A}$, $\\mathcal{E}$ and $\\lambda$ via rules\n",
    "\n",
    "Also:\n",
    "- Blei [notes](https://www.cs.princeton.edu/courses/archive/spring12/cos424/pdf/em-mixtures.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Say the data is generated via a mixture of multinomials. (It could also be a mixture of HMMs, etc). \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from numpy.random import multinomial\n",
    "from numpy.random import beta\n",
    "\n",
    "\n",
    "# data generation procedure\n",
    "V = range(11)  # vocab of size 10\n",
    "\n",
    "WORDSPERDOC = 100  # 100 words per doc\n",
    "NDOCS = 14\n",
    "alpha = [random.randint(1,5) for i in V]\n",
    "alpha2 = [random.randint(1,5) for i in V]\n",
    "a,b = [random.randint(1,5) for i in range(2)]\n",
    "\n",
    "pi = beta(a,b)\n",
    "group1 = np.random.dirichlet(alpha)\n",
    "group2 = np.random.dirichlet(alpha2)\n",
    "theta = np.vstack([group1, group2])\n",
    "\n",
    "docs = np.zeros((NDOCS, len(V)))\n",
    "lambda_ = np.zeros(NDOCS,)\n",
    "\n",
    "for dno, d in enumerate(range(NDOCS)):\n",
    "    if random.uniform(0, 1) < pi:\n",
    "        w1 = multinomial(WORDSPERDOC, group1)\n",
    "        docs[dno] = w1\n",
    "        lambda_[dno] = 0\n",
    "    else:\n",
    "        w2 = multinomial(WORDSPERDOC, group2)\n",
    "        docs[dno] = w2\n",
    "        lambda_[dno] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-209.22880375713453 -2845.4179912047252\n",
      "-3054.64679496186\n",
      "-366.9677219313346 -2642.782780389375\n",
      "-3009.7505023207095\n",
      "-571.8902709103461 -2381.8105576628004\n",
      "-2953.7008285731463\n",
      "-571.9055751413597 -2381.790073483815\n",
      "-2953.6956486251747\n",
      "-571.9055751413597 -2381.790073483815\n",
      "-2953.6956486251747\n",
      "-571.9055751413597 -2381.790073483815\n",
      "-2953.6956486251747\n",
      "-571.9055751413597 -2381.790073483815\n",
      "-2953.6956486251747\n",
      "-571.9055751413597 -2381.790073483815\n",
      "-2953.6956486251747\n",
      "-571.9055751413597 -2381.790073483815\n",
      "-2953.6956486251747\n",
      "-571.9055751413597 -2381.790073483815\n",
      "-2953.6956486251747\n"
     ]
    }
   ],
   "source": [
    "# init params\n",
    "theta_hat = np.random.rand(2, len(V))\n",
    "theta_hat /= np.sum(theta_hat, axis=1).reshape(-1, 1)\n",
    "pi_hat = np.random.uniform(0,1)\n",
    "lambda_hat = np.random.rand(1, NDOCS)\n",
    "\n",
    "for i in range(10):\n",
    "    # estep\n",
    "    d1 = np.exp(np.sum((docs * np.log(theta_hat[0])), axis=1) + np.log(pi_hat))\n",
    "    d2 = np.exp(np.sum((docs * np.log(theta_hat[1])), axis=1) + np.log(1 - pi_hat))\n",
    "    lambda_hat = (d1/(d1 + d2))\n",
    "\n",
    "    # mstep\n",
    "    \n",
    "    #pi\n",
    "    pi_hat = np.sum(lambda_hat)/NDOCS\n",
    "\n",
    "    # theta 0\n",
    "    expected_counts_under_assignments = lambda_hat.reshape(-1,1) * docs\n",
    "    d = np.sum(expected_counts_under_assignments)\n",
    "    n = np.sum(expected_counts_under_assignments,axis=0)\n",
    "    theta_hat[0] = n/d\n",
    "\n",
    "    # theta 1\n",
    "    expected_counts_under_assignments = (1 - lambda_hat).reshape(-1,1) * docs\n",
    "    d = np.sum(expected_counts_under_assignments)\n",
    "    n = np.sum(expected_counts_under_assignments,axis=0)\n",
    "    theta_hat[1] = n/d\n",
    "\n",
    "    # get nll\n",
    "    a = np.sum((lambda_hat.reshape(-1,1) * docs) * np.log(theta_hat[0]))\n",
    "    b = np.sum(((1 - lambda_hat).reshape(-1,1) * docs) * np.log(theta_hat[1]))\n",
    "    print(a, b)\n",
    "    print(a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_hat[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
