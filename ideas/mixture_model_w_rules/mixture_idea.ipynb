{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A researcher wants a generative model of entities and their attributes. The data takes the form of $\\mathcal{D}_{\\lambda} = [(e_1,a_1),(e_2,a_2) ... (e_N,a_N)] $, where $\\mathcal{\\lambda}$ is an extraction strategy, $\\mathcal{A}$ is an attribute set and $\\mathcal{E}$ is an entity set. Note that $\\lambda$ defines $\\mathcal{D}_{\\lambda}$.\n",
    "\n",
    "There are lots of reasons to prefer a generative model of $\\mathcal{D}_{\\lambda}$\n",
    "1. testing hypothesis about $(e,a)$ pairs with likelihood ratios \n",
    "2. quantifying uncertainty about conclusions with credible intervals\n",
    "3. transparently incorporating prior beliefs about entities and their attributes \n",
    "     - possibly even including prior beliefs about extraction strategies $\\lambda$, entity sets $\\mathcal{E}$ and attribute sets $\\mathcal{A}$\n",
    "4. updating priors in the face of evidence\n",
    "5. suggesting new interpretations of the data (e.g. suggesting new $a$ for inclusion in $\\mathcal{A}$, or new $e$ for inclusion in $\\mathcal{E}$)\n",
    "\n",
    "A researcher expresses their beliefs about $\\mathcal{A}$, $\\mathcal{E}$ and $\\lambda$ via rules\n",
    "\n",
    "Also:\n",
    "- Blei [notes](https://www.cs.princeton.edu/courses/archive/spring12/cos424/pdf/em-mixtures.pdf)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "Title: Sort rules for entity-attribute analysis\n",
    "\n",
    "Basic idea: use snorkel rules define E and A, which are typically unknown at the start of the process. \n",
    "- What are the Ys? The Ys are the zs: i.e. is (a, e, m) ? That is just now known at the moment. \n",
    "- Let $(e,a,m)$ be an entity, attribute, mention triple. Let $z$ be a variable expressing: does $a$ refer to a permanent property of $e$ in $m$? \n",
    "    - $z$ is analogous to $y$ in snorkel.\n",
    "    - $a$ is analogous to a version of $\\lambda$ in snorkel. \n",
    "    - For now, assume $E$ is known ahead of time.\n",
    "\n",
    "- Suggest other rules. Incorporate feedback from researcher into a soft analysis system.\n",
    "- Rules could be generated from a regular expression, like sipser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Say the data is generated via a mixture of multinomials. (It could also be a mixture of HMMs, etc). \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "import random\n",
    "from numpy.random import multinomial\n",
    "from numpy.random import beta\n",
    "\n",
    "\n",
    "# data generation procedure\n",
    "V = range(11)  # vocab of size 10\n",
    "\n",
    "WORDSPERDOC = 100  # 100 words per doc\n",
    "NDOCS = 21\n",
    "alpha = [random.randint(1,6) for i in V]\n",
    "alpha2 = [random.randint(1,6) for i in V]\n",
    "a,b = [random.randint(1,6) for i in range(2)]\n",
    "\n",
    "pi = beta(a,b)\n",
    "group1 = np.random.dirichlet(alpha)\n",
    "group2 = np.random.dirichlet(alpha2)\n",
    "theta = np.vstack([group1, group2])\n",
    "\n",
    "docs = np.zeros((NDOCS, len(V)))\n",
    "lambda_ = np.zeros(NDOCS,)\n",
    "\n",
    "for dno, d in enumerate(range(NDOCS)):\n",
    "    if random.uniform(0, 1) < pi:\n",
    "        w1 = multinomial(WORDSPERDOC, group1)\n",
    "        docs[dno] = w1\n",
    "        lambda_[dno] = 0\n",
    "    else:\n",
    "        w2 = multinomial(WORDSPERDOC, group2)\n",
    "        docs[dno] = w2\n",
    "        lambda_[dno] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4899.849849071947\n",
      "-4585.374550037082\n",
      "-4517.025288795171\n",
      "-4517.025288795171\n",
      "-4517.025288795171\n",
      "-4517.025288795171\n",
      "-4517.025288795171\n",
      "-4517.025288795171\n",
      "-4517.025288795171\n",
      "-4517.025288795171\n"
     ]
    }
   ],
   "source": [
    "# init params\n",
    "theta_hat = np.random.rand(2, len(V))\n",
    "theta_hat /= np.sum(theta_hat, axis=1).reshape(-1, 1)\n",
    "pi_hat = np.random.uniform(0,1)\n",
    "lambda_hat = np.random.rand(1, NDOCS)\n",
    "\n",
    "for i in range(10):\n",
    "    # estep\n",
    "    d1 = np.exp(np.sum((docs * np.log(theta_hat[0])), axis=1) + np.log(pi_hat))\n",
    "    d2 = np.exp(np.sum((docs * np.log(theta_hat[1])), axis=1) + np.log(1 - pi_hat))\n",
    "    lambda_hat = (d1/(d1 + d2))\n",
    "\n",
    "    # mstep\n",
    "    \n",
    "    #pi\n",
    "    pi_hat = np.sum(lambda_hat)/NDOCS\n",
    "\n",
    "    # theta 0\n",
    "    expected_counts_under_assignments = lambda_hat.reshape(-1,1) * docs\n",
    "    d = np.sum(expected_counts_under_assignments)\n",
    "    n = np.sum(expected_counts_under_assignments,axis=0)\n",
    "    theta_hat[0] = n/d\n",
    "\n",
    "    # theta 1\n",
    "    expected_counts_under_assignments = (1 - lambda_hat).reshape(-1,1) * docs\n",
    "    d = np.sum(expected_counts_under_assignments)\n",
    "    n = np.sum(expected_counts_under_assignments,axis=0)\n",
    "    theta_hat[1] = n/d\n",
    "\n",
    "    # get nll\n",
    "    a = np.sum((lambda_hat.reshape(-1,1) * docs) * np.log(theta_hat[0]))\n",
    "    b = np.sum(((1 - lambda_hat).reshape(-1,1) * docs) * np.log(theta_hat[1]))\n",
    "    #print(a, b)\n",
    "    print(a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import elementwise_grad as egrad\n",
    "# https://github.com/HIPS/autograd\n",
    "\n",
    "def tanh(x):                 # Define a function\n",
    "    y = np.exp(-2.0 * x)\n",
    "    return (1.0 - y) / (1.0 + y)\n",
    "\n",
    "\n",
    "def eq1_simple(a,b,lambda_,y):\n",
    "    M = len(lambda_)\n",
    "    ou = 1\n",
    "    for m in range(M):\n",
    "        ou *= b[m] * a[m] * (lambda_[m] == y[m])\n",
    "        ou *= b[m] * (1 - a[m]) * (lambda_[m] == -1 * y[m])\n",
    "        ou *= (1 - b[m]) * (lambda_[m] == 0)\n",
    "    return ou\n",
    "    \n",
    "\n",
    "grad_tanh = egrad(tanh)        # Obtain its gradient function\n",
    "grad_tanh(np.asarray([1.,2.])) # Evaluate the gradient at x = 1.0\n",
    "grad = egrad(eq1_simple)\n",
    "\n",
    "\n",
    "# what are the attributes? what are the entities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "breeds = [\"pitbull\", \"lab\", \"golden\"]\n",
    "attributes = [\"cute\"]\n",
    "\n",
    "emissions = defaultdict(list)\n",
    "stream = []\n",
    "\n",
    "import string \n",
    "punct = [i for i in string.punctuation]\n",
    "\n",
    "with open(\"dogs.subreddit.mini.jsonl\", \"r\") as inf:\n",
    "    for i in inf:\n",
    "        i = json.loads(i)\n",
    "        stream.append(\"SOS\")\n",
    "        for t in i[\"tokens\"]:\n",
    "            if t[\"word\"] not in ['\\\\', 'n', '\"', \"''\"] + punct:\n",
    "                pos = t[\"pos\"]\n",
    "                if t[\"word\"].lower() in breeds:\n",
    "                    pos = \"E\"\n",
    "                if t[\"word\"].lower() in attributes:\n",
    "                    pos = \"A\"\n",
    "                emissions[pos].append(t[\"word\"].lower())\n",
    "                stream.append(pos)\n",
    "        stream.append(\"EOS\")\n",
    "        \n",
    "transitions = defaultdict(list)\n",
    "for sno in range(len(stream) - 1):\n",
    "    s1,s2 = stream[sno:sno+2]\n",
    "    transitions[s1].append(s2)\n",
    "\n",
    "states = set(transitions)\n",
    "V = set(j for i in emissions.values() for j in i)\n",
    "s2n = {v:k for k,v in enumerate(states)}\n",
    "v2n = {v:k for k,v in enumerate(V)}\n",
    "n2v = {k:v for k,v in enumerate(v2n)}\n",
    "n2s = {k:v for k,v in enumerate(s2n)}\n",
    "\n",
    "tprobs = np.zeros((len(states), len(states)))\n",
    "eprobs = np.zeros((len(states), len(V)))\n",
    "\n",
    "for t in transitions:\n",
    "    tot = len(transitions[t])\n",
    "    observed = Counter(transitions[t])\n",
    "    for s2,count in observed.items():\n",
    "        tprobs[s2n[t]][s2n[s2]] = count/tot\n",
    "\n",
    "for state in emissions:\n",
    "    tot = len(emissions[state])\n",
    "    observed = Counter(emissions[state])\n",
    "    for em, count in observed.items():\n",
    "        eprobs[s2n[state]][v2n[em]] = count/tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cute\n"
     ]
    }
   ],
   "source": [
    "state = \"A\"\n",
    "\n",
    "def sample_next_state(state):\n",
    "    return n2s[np.nonzero(np.random.multinomial(1, tprobs[s2n[state]]))[0][0]]\n",
    "\n",
    "def sample_emission(state):\n",
    "    return n2v[np.nonzero(np.random.multinomial(1, eprobs[s2n[state]]))[0][0]]\n",
    "\n",
    "for i in range(3):\n",
    "    if state == \"EOS\":\n",
    "        break\n",
    "    em = sample_emission(state)\n",
    "    state = sample_next_state(state)\n",
    "    print(em,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PRP'"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define\n",
    "2. grep\n",
    "3. train\n",
    "4. sample\n",
    "5. define"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
