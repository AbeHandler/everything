{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A researcher wants a generative model of entities and their attributes. The data takes the form of $\\mathcal{D}_{\\lambda} = [(e_1,a_1),(e_2,a_2) ... (e_N,a_N)] $, where $\\mathcal{\\lambda}$ is an extraction strategy, $\\mathcal{A}$ is an attribute set and $\\mathcal{E}$ is an entity set. Note that $\\lambda$ defines $\\mathcal{D}_{\\lambda}$.\n",
    "\n",
    "There are lots of reasons to prefer a generative model of $\\mathcal{D}_{\\lambda}$\n",
    "1. testing hypothesis about $(e,a)$ pairs with likelihood ratios \n",
    "2. quantifying uncertainty about conclusions with credible intervals\n",
    "3. transparently incorporating prior beliefs about entities and their attributes \n",
    "     - possibly even including prior beliefs about extraction strategies $\\lambda$, entity sets $\\mathcal{E}$ and attribute sets $\\mathcal{A}$\n",
    "4. updating priors in the face of evidence\n",
    "5. suggesting new interpretations of the data (e.g. suggesting new $a$ for inclusion in $\\mathcal{A}$, or new $e$ for inclusion in $\\mathcal{E}$)\n",
    "\n",
    "A researcher expresses their beliefs about $\\mathcal{A}$, $\\mathcal{E}$ and $\\lambda$ via rules\n",
    "\n",
    "Also:\n",
    "- Blei [notes](https://www.cs.princeton.edu/courses/archive/spring12/cos424/pdf/em-mixtures.pdf)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "Title: Sort rules for entity-attribute analysis\n",
    "\n",
    "Basic idea: use snorkel rules define E and A, which are typically unknown at the start of the process. \n",
    "- What are the Ys? The Ys are the zs: i.e. is (a, e, m) ? That is just now known at the moment. \n",
    "- Let $(e,a,m)$ be an entity, attribute, mention triple. Let $z$ be a variable expressing: does $a$ refer to a permanent property of $e$ in $m$? \n",
    "    - $z$ is analogous to $y$ in snorkel.\n",
    "    - $a$ is analogous to a version of $\\lambda$ in snorkel. \n",
    "    - For now, assume $E$ is known ahead of time.\n",
    "\n",
    "- Suggest other rules. Incorporate feedback from researcher into a soft analysis system.\n",
    "- Rules could be generated from a regular expression, like sipser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Say the data is generated via a mixture of multinomials. (It could also be a mixture of HMMs, etc). \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "import random\n",
    "from numpy.random import multinomial\n",
    "from numpy.random import beta\n",
    "\n",
    "\n",
    "# data generation procedure\n",
    "V = range(11)  # vocab of size 10\n",
    "\n",
    "WORDSPERDOC = 100  # 100 words per doc\n",
    "NDOCS = 21\n",
    "alpha = [random.randint(1,6) for i in V]\n",
    "alpha2 = [random.randint(1,6) for i in V]\n",
    "a,b = [random.randint(1,6) for i in range(2)]\n",
    "\n",
    "pi = beta(a,b)\n",
    "group1 = np.random.dirichlet(alpha)\n",
    "group2 = np.random.dirichlet(alpha2)\n",
    "theta = np.vstack([group1, group2])\n",
    "\n",
    "docs = np.zeros((NDOCS, len(V)))\n",
    "lambda_ = np.zeros(NDOCS,)\n",
    "\n",
    "for dno, d in enumerate(range(NDOCS)):\n",
    "    if random.uniform(0, 1) < pi:\n",
    "        w1 = multinomial(WORDSPERDOC, group1)\n",
    "        docs[dno] = w1\n",
    "        lambda_[dno] = 0\n",
    "    else:\n",
    "        w2 = multinomial(WORDSPERDOC, group2)\n",
    "        docs[dno] = w2\n",
    "        lambda_[dno] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4899.849849071947\n",
      "-4585.374550037082\n",
      "-4517.025288795171\n",
      "-4517.025288795171\n",
      "-4517.025288795171\n",
      "-4517.025288795171\n",
      "-4517.025288795171\n",
      "-4517.025288795171\n",
      "-4517.025288795171\n",
      "-4517.025288795171\n"
     ]
    }
   ],
   "source": [
    "# init params\n",
    "theta_hat = np.random.rand(2, len(V))\n",
    "theta_hat /= np.sum(theta_hat, axis=1).reshape(-1, 1)\n",
    "pi_hat = np.random.uniform(0,1)\n",
    "lambda_hat = np.random.rand(1, NDOCS)\n",
    "\n",
    "for i in range(10):\n",
    "    # estep\n",
    "    d1 = np.exp(np.sum((docs * np.log(theta_hat[0])), axis=1) + np.log(pi_hat))\n",
    "    d2 = np.exp(np.sum((docs * np.log(theta_hat[1])), axis=1) + np.log(1 - pi_hat))\n",
    "    lambda_hat = (d1/(d1 + d2))\n",
    "\n",
    "    # mstep\n",
    "    \n",
    "    #pi\n",
    "    pi_hat = np.sum(lambda_hat)/NDOCS\n",
    "\n",
    "    # theta 0\n",
    "    expected_counts_under_assignments = lambda_hat.reshape(-1,1) * docs\n",
    "    d = np.sum(expected_counts_under_assignments)\n",
    "    n = np.sum(expected_counts_under_assignments,axis=0)\n",
    "    theta_hat[0] = n/d\n",
    "\n",
    "    # theta 1\n",
    "    expected_counts_under_assignments = (1 - lambda_hat).reshape(-1,1) * docs\n",
    "    d = np.sum(expected_counts_under_assignments)\n",
    "    n = np.sum(expected_counts_under_assignments,axis=0)\n",
    "    theta_hat[1] = n/d\n",
    "\n",
    "    # get nll\n",
    "    a = np.sum((lambda_hat.reshape(-1,1) * docs) * np.log(theta_hat[0]))\n",
    "    b = np.sum(((1 - lambda_hat).reshape(-1,1) * docs) * np.log(theta_hat[1]))\n",
    "    #print(a, b)\n",
    "    print(a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import elementwise_grad as egrad\n",
    "# https://github.com/HIPS/autograd\n",
    "\n",
    "def tanh(x):                 # Define a function\n",
    "    y = np.exp(-2.0 * x)\n",
    "    return (1.0 - y) / (1.0 + y)\n",
    "\n",
    "\n",
    "def eq1_simple(a,b,lambda_,y):\n",
    "    M = len(lambda_)\n",
    "    ou = 1\n",
    "    for m in range(M):\n",
    "        ou *= b[m] * a[m] * (lambda_[m] == y[m])\n",
    "        ou *= b[m] * (1 - a[m]) * (lambda_[m] == -1 * y[m])\n",
    "        ou *= (1 - b[m]) * (lambda_[m] == 0)\n",
    "    return ou\n",
    "    \n",
    "\n",
    "grad_tanh = egrad(tanh)        # Obtain its gradient function\n",
    "grad_tanh(np.asarray([1.,2.])) # Evaluate the gradient at x = 1.0\n",
    "grad = egrad(eq1_simple)\n",
    "\n",
    "\n",
    "# what are the attributes? what are the entities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "breeds = [\"pitbull\", \"lab\", \"golden\"]\n",
    "attributes = [\"aggressive\"]\n",
    "\n",
    "emissions = defaultdict(list)\n",
    "stream = []\n",
    "\n",
    "import string \n",
    "punct = [i for i in string.punctuation]\n",
    "\n",
    "with open(\"dogs.subreddit.mini.jsonl\", \"r\") as inf:\n",
    "    for i in inf:\n",
    "        i = json.loads(i)\n",
    "        words = [w[\"word\"] for w in i['tokens']]\n",
    "        if len(set(words) & set(breeds + attributes)) > 0:\n",
    "            stream.append(\"SOS\")\n",
    "            for t in i[\"tokens\"]:\n",
    "                if t[\"word\"] not in ['\\\\', 'n', '\"', \"''\"] + punct:\n",
    "                    pos = t[\"pos\"]\n",
    "                    if t[\"word\"].lower() in breeds:\n",
    "                        pos = \"E\"\n",
    "                    if t[\"word\"].lower() in attributes:\n",
    "                        pos = \"A\"\n",
    "                    emissions[pos].append(t[\"word\"].lower())\n",
    "                    stream.append(pos)\n",
    "            stream.append(\"EOS\")\n",
    "        \n",
    "transitions = defaultdict(list)\n",
    "for sno in range(len(stream) - 1):\n",
    "    s1,s2 = stream[sno:sno+2]\n",
    "    transitions[s1].append(s2)\n",
    "\n",
    "states = set(transitions)\n",
    "V = set(j for i in emissions.values() for j in i)\n",
    "s2n = {v:k for k,v in enumerate(states)}\n",
    "v2n = {v:k for k,v in enumerate(V)}\n",
    "n2v = {k:v for k,v in enumerate(v2n)}\n",
    "n2s = {k:v for k,v in enumerate(s2n)}\n",
    "\n",
    "tprobs = np.zeros((len(states), len(states)))\n",
    "eprobs = np.zeros((len(states), len(V)))\n",
    "\n",
    "for t in transitions:\n",
    "    tot = len(transitions[t])\n",
    "    observed = Counter(transitions[t])\n",
    "    for s2,count in observed.items():\n",
    "        tprobs[s2n[t]][s2n[s2]] = count/tot\n",
    "\n",
    "for state in emissions:\n",
    "    tot = len(emissions[state])\n",
    "    observed = Counter(emissions[state])\n",
    "    for em, count in observed.items():\n",
    "        eprobs[s2n[state]][v2n[em]] = count/tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next_state(state):\n",
    "    return n2s[np.nonzero(np.random.multinomial(1, tprobs[s2n[state]]))[0][0]]\n",
    "\n",
    "def sample_emission(state):\n",
    "    return n2v[np.nonzero(np.random.multinomial(1, eprobs[s2n[state]]))[0][0]]\n",
    "\n",
    "from random import  randint\n",
    "\n",
    "# generate from the model. This is equivalent to forward MC search over the WFSA lattice \n",
    "\n",
    "patterns = []\n",
    "\n",
    "for i in range(1000):\n",
    "    state = \"A\"\n",
    "    K = randint(2, 6)\n",
    "    pattern = \" \"\n",
    "    for i in range(K):\n",
    "        em = sample_emission(state)\n",
    "        state = sample_next_state(state)\n",
    "        if state == \"EOS\":\n",
    "            break\n",
    "        pattern = pattern + ' ' + em\n",
    "    patterns.append(pattern.strip())\n",
    "patterns = set([i for i in patterns if len(i.split(\" \")) > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggressive breed\n",
      "the jaw - this technique can be used with all dogs but pitties in particular - check out badrap dog training \\ n \\ nAnyways bottom line is this - the queensland / austrialian cattle dog is a slightly aggressive breed - they are kick ass dogs who are brilliant and loyal but they typically are a family dog who is only cuddly once they know you .\n",
      "aggressive but\n",
      "The Austrialian shepard is a less aggressive but they can be .\n",
      "aggressive but the\n",
      "The Austrialian shepard is a less aggressive but they can be .\n",
      "aggressive i\n",
      "He 's learned from past experience that if he gets too aggressive in his warning/correction of her , I will take the resource away .\n",
      "aggressive in\n",
      "He 's learned from past experience that if he gets too aggressive in his warning/correction of her , I will take the resource away .\n",
      "aggressive or\n",
      "NO belgian should be aggressive or hostile in normal life .\n",
      "aggressive or a\n",
      "aying she 's for-sure fear aggressive or anything , but the growling is a little worrisome on that end and I just would n't push her too hard these first few weeks .\n",
      "aggressive or\n",
      "aying she 's for-sure fear aggressive or anything , but the growling is a little worrisome on that end and I just would n't push her too hard these first few weeks .\n",
      "aggressive towards\n",
      "That 's my opinion on it bud , I would n't let anyone who is in the slightest aggressive towards dogs into my home .\n",
      "aggressive dog\n",
      "You think he 'd do that to an aggressive dog ?\n",
      "aggressive me\n",
      "So it ends up an anxious , reactive , loud , aggressive mess .\n",
      "aggressive and\n",
      "They did not tell me the dog was aggressive and she ended up attacking a few people -LRB- completely unprovoked -RRB- .\n"
     ]
    }
   ],
   "source": [
    "with open(\"dogs.subreddit.mini.jsonl\", \"r\") as inf:\n",
    "    for i in inf:\n",
    "        i = json.loads(i)\n",
    "        words = \" \".join([o['word'] for o in i['tokens']])\n",
    "        for p in patterns:\n",
    "            if p in words:\n",
    "                print(p)\n",
    "                ix = words.index(p)\n",
    "                print(words[ix-200:ix+200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic process\n",
    "\n",
    "1. Define\n",
    "2. grep\n",
    "3. train\n",
    "4. sample\n",
    "5. define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is different about this? What is new here?\n",
    "\n",
    "1. human in the loop dipre\n",
    "2. summarization problems are new angle also\n",
    "3. You could sell this as intretable rules for manual coding.\n",
    "      - say you have entities and known gendered attributes. what are the syntactic structures connecting them? You have this with Wu. Can you make it into an intpretable rule. One simple rule is a <-amod- e. Even just defining the syntactic relationships between the entities and their attributes in an interpretable way is new. How are they related. The idea is that gendered or racialized words can predict the class. This is quite vague. **why** is this racism or sexism? This is missing from the literature. One thing you could do is read. Another thing you could do is try to explain. \"Explaining racist or gendered language.\"\n",
    "      \n",
    "1. Do you know e?\n",
    "2. Do you know a? \n",
    "3. If you know e and a, then **why**. \n",
    "4. If you have such an explanation and it is good, then it should be robust to modeling choices.\n",
    "5. So you can vary lambda, and then run models and see what is the same. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
