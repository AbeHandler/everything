{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "def predict(X, w):\n",
    "    a = X.dot(w) < 0  \n",
    "    a = a.astype(int)  # will be True if less than zero\n",
    "    a[a == 1] = -1     # hence gets the label -1\n",
    "    a[a == 0] = 1      # else gets the label 1 b/c greater than 0\n",
    "    return a\n",
    "\n",
    "def accuracy(X, w, y):\n",
    "    return np.sum(predict(X, w) == y)/y.size\n",
    "\n",
    "def loss(weights, features, labels):\n",
    "    return np.sum((labels - features.dot(weights)) ** 2)\n",
    "\n",
    "def grad(weights, features, labels):\n",
    "    a =  -2 * np.multiply(features, (labels - features.dot(weights)))\n",
    "    return np.sum(a, axis=0).reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the iris dataset\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "N = 100\n",
    "X = iris.data[:, :2][0:N]  # we only take the first two features and 100 rows\n",
    "y = iris.target[0:N]       # we only take the first two features and 100 rows\n",
    " \n",
    "N, F = X.shape\n",
    "\n",
    "y = y.reshape(N,1)\n",
    "\n",
    "y[y == 0] = -1\n",
    "\n",
    "# Initialize the weights\n",
    "\n",
    "w = np.random.rand(F).reshape(F,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building intuition for the loss function \n",
    "\n",
    "Describe the loss function for Adeline in your own words. What does the equation say? \n",
    "\n",
    "[Type your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the existing code\n",
    "\n",
    "Using the slides on Adeline as a guide, please examine the `predict`, `accuracy`, `loss` and `grad` functions above. Please describe in your own words what each function is doing \n",
    "\n",
    "[Type your answer here, describing each function]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "The gradient points in the direction of greatest increase of a function. \n",
    "If we want to train Adeline, we should use gradient *descent*, which takes steps in the *opposite* direction of the gradient. Why does make sense, given the Adeline loss function \n",
    "\n",
    "[Type your answer here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n",
      "56.838158304043205\n"
     ]
    }
   ],
   "source": [
    "ETA = .0001\n",
    "ITERS = 100\n",
    "\n",
    "# Using the slide on Gradient descent as a guide, \n",
    "# finish the implementation of gradient descent below\n",
    "\n",
    "for i in range(ITERS):\n",
    "    # implement me!\n",
    "    print(loss(weights=w, features=X, labels=y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss by iteration\n",
    "\n",
    "Plot the loss at each iteration of the algorithm. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy by loss \n",
    "\n",
    "Use the `accuracy` function to measure the accuracy at each iteration of the algorithm. What do you observe about the relationship between the accuracy and the loss? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rates\n",
    "\n",
    "Try varing the learning rate eta by increasing or decreasing eta by powers of 10. Make a plot showing the learning rate and the accuracy after 100 iterations. Does the algorithm achieve high accuracy for all eta, or only for some learning rates? Why might this be the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Bonus: convert the numpy functions into native Python\n",
    "\n",
    "If you find yourself with extra time, try translating each of the functions at the top of the notebook into native Python functions (i.e. use for loops, instead of numpy operations). If you do this step, please send me a message on Canvas and turn in a file called `extra_credit.py` showing your implementation of one or more of the functions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
