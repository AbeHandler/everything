{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "def pred(X, w):\n",
    "    a = X.dot(w) < 0  \n",
    "    a = a.astype(int)  # will be True if less than zero\n",
    "    a[a == 1] = -1     # hence gets the label -1\n",
    "    a[a == 0] = 1      # else gets the label 1 b/c greater than 0\n",
    "    return a\n",
    "\n",
    "def acc(X, w, y):\n",
    "    return np.sum(pred(X, w) == y)/y.size\n",
    "\n",
    "def loss(weights, features, labels):\n",
    "    return np.sum((labels - features.dot(weights)) ** 2)\n",
    "\n",
    "def grad(weights, features, labels):\n",
    "    a =  -2 * np.multiply(features, (labels - features.dot(weights)))\n",
    "    return np.sum(a, axis=0).reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 194.15392737896514 0.5\n",
      "1 121.09455762635633 0.5\n",
      "2 116.84417994554457 0.22\n",
      "3 115.44759260496132 0.12\n",
      "4 114.18448764964997 0.11\n",
      "5 112.94247101758336 0.12\n",
      "6 111.71670595118404 0.12\n",
      "7 110.50679413732735 0.17\n",
      "8 109.3125228819711 0.19\n",
      "9 108.13368968340315 0.24\n",
      "10 106.97009496455688 0.35\n",
      "11 105.82154174090745 0.43\n",
      "12 104.68783557482129 0.45\n",
      "13 103.56878454213258 0.49\n",
      "14 102.46419919963165 0.5\n",
      "15 101.37389255299341 0.5\n",
      "16 100.2976800251212 0.5\n",
      "17 99.23537942489989 0.5\n",
      "18 98.18681091635281 0.5\n",
      "19 97.15179698819776 0.5\n",
      "20 96.1301624237959 0.5\n",
      "21 95.12173427148971 0.54\n",
      "22 94.12634181532407 0.64\n",
      "23 93.14381654614591 0.71\n",
      "24 92.17399213307743 0.81\n",
      "25 91.21670439535819 0.84\n",
      "26 90.27179127455082 0.87\n",
      "27 89.33909280710664 0.89\n",
      "28 88.41845109728524 0.91\n",
      "29 87.50971029042465 0.95\n",
      "30 86.61271654655678 0.97\n",
      "31 85.72731801436393 0.97\n",
      "32 84.85336480547204 0.97\n",
      "33 83.99070896907627 0.97\n",
      "34 83.13920446689445 0.98\n",
      "35 82.29870714844442 0.98\n",
      "36 81.46907472664097 0.98\n",
      "37 80.65016675370796 0.98\n",
      "38 79.84184459740241 0.98\n",
      "39 79.04397141754517 0.98\n",
      "40 78.25641214285561 0.99\n",
      "41 77.47903344808542 0.99\n",
      "42 76.71170373144822 0.99\n",
      "43 75.95429309234052 0.99\n",
      "44 75.20667330935129 0.99\n",
      "45 74.46871781855525 0.99\n",
      "46 73.74030169208699 0.99\n",
      "47 73.02130161699215 0.99\n",
      "48 72.31159587435181 0.99\n",
      "49 71.61106431867665 0.99\n",
      "50 70.91958835756795 0.99\n",
      "51 70.23705093164084 0.99\n",
      "52 69.56333649470761 0.99\n",
      "53 68.89833099421703 0.99\n",
      "54 68.24192185194624 0.99\n",
      "55 67.59399794494281 0.99\n",
      "56 66.95444958671257 0.99\n",
      "57 66.32316850865098 0.99\n",
      "58 65.70004784171442 0.99\n",
      "59 65.08498209832845 0.99\n",
      "60 64.47786715452985 0.99\n",
      "61 63.87860023233958 0.99\n",
      "62 63.287079882363585 0.99\n",
      "63 62.70320596661854 0.99\n",
      "64 62.126879641579514 0.99\n",
      "65 61.55800334144693 0.99\n",
      "66 60.99648076162957 0.99\n",
      "67 60.44221684244144 0.99\n",
      "68 59.89511775300911 0.99\n",
      "69 59.35509087538711 0.99\n",
      "70 58.82204478887879 0.99\n",
      "71 58.29588925455978 0.99\n",
      "72 57.7765352000015 0.99\n",
      "73 57.26389470419212 0.99\n",
      "74 56.75788098265263 0.99\n",
      "75 56.25840837274499 0.99\n",
      "76 55.76539231917049 0.99\n",
      "77 55.2787493596554 0.99\n",
      "78 54.79839711082175 0.99\n",
      "79 54.32425425424069 0.99\n",
      "80 53.85624052266618 0.99\n",
      "81 53.39427668644659 0.99\n",
      "82 52.938284540111994 0.99\n",
      "83 52.48818688913488 0.99\n",
      "84 52.04390753686182 0.99\n",
      "85 51.605371271614345 0.99\n",
      "86 51.17250385395637 0.99\n",
      "87 50.745232004126244 0.99\n",
      "88 50.32348338963128 0.99\n",
      "89 49.90718661300268 0.99\n",
      "90 49.496271199708715 0.99\n",
      "91 49.09066758622409 0.99\n",
      "92 48.690307108253684 0.99\n",
      "93 48.29512198910838 0.99\n",
      "94 47.90504532823119 0.99\n",
      "95 47.52001108987174 0.99\n",
      "96 47.139954091907136 0.99\n",
      "97 46.76480999480734 0.99\n",
      "98 46.39451529074315 0.99\n",
      "99 46.02900729283508 0.99\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "N = 100\n",
    "X = iris.data[:, :2][0:N]  # we only take the first two features and 100 rows\n",
    "y = iris.target[0:N] # we only take the first two features and 100 rows\n",
    " \n",
    "N, F = X.shape\n",
    "\n",
    "y = y.reshape(N,1)\n",
    "\n",
    "y[y == 0] = -1\n",
    "\n",
    "w = np.random.rand(F).reshape(F,1)\n",
    "\n",
    "w = np.asarray([[.5, .5]]).reshape(F,1)\n",
    "\n",
    "for i in range(100):\n",
    "    w -= .0001 * grad(w, X, y)\n",
    "    print(i, loss(w, X, y), acc(X, w, y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with different learning rates, does it converge and why \n",
    "\n",
    "### Plot accuracy vs loss\n",
    "\n",
    "### Bonus: convert the numpy functions into python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
