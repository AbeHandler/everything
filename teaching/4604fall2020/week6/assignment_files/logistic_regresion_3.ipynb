{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CU Boulder 4604/5604\n",
    "\n",
    "#### September 28, 2020\n",
    "\n",
    "####  Logistic regression revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression revisited\n",
    "\n",
    "- Last week we learned about logistic regression and regularization\n",
    "- Today, we are going modify the notebook from last week in 2 ways \n",
    "\n",
    "##### New stuff:\n",
    "1. I filled in the missing functions from last week if you are curious. Everyone who submitted last week got full credit. These notebooks are a way to get your hands dirty with real ML code so you learn the materials more deeply. They are not a way for me to quiz you on what you know. That is what HW, quizzes, exams are for ...\n",
    "2. We are going to add a regularization term to the loss function\n",
    "3. We will split training data and test data, and look at the effects of regularization\n",
    "4. We will introduce stochastic gradient descent. The well-known ML researcher [Dave Blei](http://www.cs.columbia.edu/~blei/) says stoachstic gradient descent is like walking from New York to Los Angeles by asking one person at a time for directions. And every person you ask for directions is drunk. Even though this seems like a bad way to get across the country, we will see that is has clear advantages\n",
    "5. I changed the code from gradient ascent to gradient descent (flipping signs as needed). Recall that maximizing log likelihood is the same as minimizing negative log likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annoucements \n",
    "\n",
    "- Recall HW2 is out\n",
    "- This notebook is pretty similar to HW2\n",
    "- In the real world, it is OK to lean on sklearn's implementation of logistic regression\n",
    "- But taking this class gives you a chance to actually understand how it works. This notebook should make the sklearn code much less mysterious"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "### L2 Norm\n",
    "$\\sqrt{\\Sigma x_i^2}$\n",
    "\n",
    "### L2$^2$ Norm \n",
    "\n",
    "$\\sqrt{\\Sigma x_i^2}^2 = \\Sigma x_i^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Here is the logistic function, the activation function for logistic regression\n",
    "\n",
    "def logistic(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "\n",
    "## Let's generate some features, weights and y values and compute the loss \n",
    "## This can help you build up intuition for what is happening\n",
    "\n",
    "def log_prob(z, y_i):\n",
    "    '''\n",
    "    Returns the log_prob for one point\n",
    "    '''\n",
    "    fz = logistic(z)\n",
    "    return y_i * np.log(fz) + (1 - y_i) * np.log(1 - fz)\n",
    "\n",
    "\n",
    "out = []\n",
    "\n",
    "dim_ = 10\n",
    "\n",
    "for _ in range(1000):\n",
    "    # generate some random weights \n",
    "    w = np.random.uniform(low=-2, high=2, size=dim_)\n",
    "    \n",
    "    # generate some random binary features \n",
    "    x = (np.random.rand(dim_) > .5).astype(int) \n",
    "    \n",
    "    # get the z score\n",
    "    z = w.dot(x)\n",
    "\n",
    "    # randomly assign y\n",
    "    y = 1 if random.random() < .5 else 0\n",
    "    \n",
    "    # compute the loss\n",
    "    loss = log_prob(z=z, y_i=y)\n",
    "    \n",
    "    # keep track of what is happening\n",
    "    out.append({\"z\": z, \"loss\": loss, \"label\": y})\n",
    "\n",
    "\n",
    "# Plot the results\n",
    "df = pd.DataFrame(out)\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(df[df[\"label\"] == 1][\"z\"], df[df[\"label\"] == 1][\"loss\"], 'x', color=\"red\", label='1')\n",
    "ax.plot(df[df[\"label\"] == 0][\"z\"], df[df[\"label\"] == 0][\"loss\"], 'o', color=\"blue\", label='0')\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"log_prob\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, loss: 1177.2101977547838, accuracy: 0.4014084507042254\n",
      "iter: 1, loss: 208.29016510389178, accuracy: 0.778169014084507\n",
      "iter: 2, loss: 614.7044694692763, accuracy: 0.4014084507042254\n",
      "iter: 3, loss: 750.4479476300494, accuracy: 0.6408450704225352\n",
      "iter: 4, loss: 1224.0838741120165, accuracy: 0.4014084507042254\n",
      "iter: 5, loss: 173.7920665149241, accuracy: 0.8098591549295775\n",
      "iter: 6, loss: 403.79325120132216, accuracy: 0.40492957746478875\n",
      "iter: 7, loss: 831.403870206119, accuracy: 0.6373239436619719\n",
      "iter: 8, loss: 1196.4143313639813, accuracy: 0.4014084507042254\n",
      "iter: 9, loss: 179.6249396656115, accuracy: 0.8098591549295775\n",
      "iter: 10, loss: 425.09608974656686, accuracy: 0.40492957746478875\n",
      "iter: 11, loss: 802.5904682193302, accuracy: 0.6408450704225352\n",
      "iter: 12, loss: 1156.7733036919371, accuracy: 0.4014084507042254\n",
      "iter: 13, loss: 192.48768220155517, accuracy: 0.8028169014084507\n",
      "iter: 14, loss: 478.63514986540304, accuracy: 0.4014084507042254\n",
      "iter: 15, loss: 762.2711623235859, accuracy: 0.647887323943662\n",
      "iter: 16, loss: 1118.1756180059156, accuracy: 0.4014084507042254\n",
      "iter: 17, loss: 206.26366213459605, accuracy: 0.7992957746478874\n",
      "iter: 18, loss: 527.6380199288494, accuracy: 0.4014084507042254\n",
      "iter: 19, loss: 711.9614730354066, accuracy: 0.647887323943662\n",
      "iter: 20, loss: 1082.0918282060215, accuracy: 0.4014084507042254\n",
      "iter: 21, loss: 219.72434471409366, accuracy: 0.7922535211267606\n",
      "iter: 22, loss: 565.3765934379347, accuracy: 0.4014084507042254\n",
      "iter: 23, loss: 663.4947317946483, accuracy: 0.6514084507042254\n",
      "iter: 24, loss: 1047.4956555154483, accuracy: 0.4014084507042254\n",
      "iter: 25, loss: 233.10248225044694, accuracy: 0.7887323943661971\n",
      "iter: 26, loss: 593.6963937578032, accuracy: 0.4014084507042254\n",
      "iter: 27, loss: 620.7326585009992, accuracy: 0.6690140845070423\n",
      "iter: 28, loss: 1009.1360514259179, accuracy: 0.4014084507042254\n",
      "iter: 29, loss: 249.82680361676762, accuracy: 0.7640845070422535\n",
      "iter: 30, loss: 625.0485390387396, accuracy: 0.4014084507042254\n",
      "iter: 31, loss: 575.8550723003609, accuracy: 0.6690140845070423\n",
      "iter: 32, loss: 977.8102220721274, accuracy: 0.4014084507042254\n",
      "iter: 33, loss: 262.07720020141034, accuracy: 0.7570422535211268\n",
      "iter: 34, loss: 636.3007637387249, accuracy: 0.4014084507042254\n",
      "iter: 35, loss: 547.4107254282491, accuracy: 0.6725352112676056\n",
      "iter: 36, loss: 954.9634349459237, accuracy: 0.4014084507042254\n",
      "iter: 37, loss: 268.2904273795889, accuracy: 0.7570422535211268\n",
      "iter: 38, loss: 628.3520104211217, accuracy: 0.4014084507042254\n",
      "iter: 39, loss: 534.8105629011878, accuracy: 0.6725352112676056\n",
      "iter: 40, loss: 933.7459741176449, accuracy: 0.4014084507042254\n",
      "iter: 41, loss: 273.3515016740112, accuracy: 0.7570422535211268\n",
      "iter: 42, loss: 616.7398230393433, accuracy: 0.4014084507042254\n",
      "iter: 43, loss: 525.1265669763324, accuracy: 0.6725352112676056\n",
      "iter: 44, loss: 912.562783572311, accuracy: 0.4014084507042254\n",
      "iter: 45, loss: 278.3995516490758, accuracy: 0.7605633802816901\n",
      "iter: 46, loss: 604.751166173837, accuracy: 0.4014084507042254\n",
      "iter: 47, loss: 515.660875869198, accuracy: 0.676056338028169\n",
      "iter: 48, loss: 890.6857821324257, accuracy: 0.4014084507042254\n",
      "iter: 49, loss: 283.93002093621715, accuracy: 0.7605633802816901\n",
      "iter: 50, loss: 593.5986678572021, accuracy: 0.40492957746478875\n",
      "iter: 51, loss: 505.55970182415564, accuracy: 0.6830985915492958\n",
      "iter: 52, loss: 867.3902771887723, accuracy: 0.4014084507042254\n",
      "iter: 53, loss: 290.41841441196306, accuracy: 0.7535211267605634\n",
      "iter: 54, loss: 584.2020111626487, accuracy: 0.40492957746478875\n",
      "iter: 55, loss: 494.3459406317566, accuracy: 0.6901408450704225\n",
      "iter: 56, loss: 841.819749837, accuracy: 0.4014084507042254\n",
      "iter: 57, loss: 298.4095296397552, accuracy: 0.75\n",
      "iter: 58, loss: 577.3959234886826, accuracy: 0.40492957746478875\n",
      "iter: 59, loss: 481.73116080376155, accuracy: 0.6971830985915493\n",
      "iter: 60, loss: 812.9714475644855, accuracy: 0.4014084507042254\n",
      "iter: 61, loss: 308.5115630301168, accuracy: 0.7464788732394366\n",
      "iter: 62, loss: 573.8886758979712, accuracy: 0.40492957746478875\n",
      "iter: 63, loss: 467.5901308854694, accuracy: 0.6971830985915493\n",
      "iter: 64, loss: 779.842485610883, accuracy: 0.4014084507042254\n",
      "iter: 65, loss: 321.2575889771835, accuracy: 0.7429577464788732\n",
      "iter: 66, loss: 573.9935500949791, accuracy: 0.40492957746478875\n",
      "iter: 67, loss: 452.04056290378537, accuracy: 0.7147887323943662\n",
      "iter: 68, loss: 741.9390950706119, accuracy: 0.4014084507042254\n",
      "iter: 69, loss: 336.67902831454523, accuracy: 0.7359154929577465\n",
      "iter: 70, loss: 577.0118347654148, accuracy: 0.40492957746478875\n",
      "iter: 71, loss: 435.6943671638019, accuracy: 0.721830985915493\n",
      "iter: 72, loss: 700.2356708218562, accuracy: 0.40492957746478875\n",
      "iter: 73, loss: 353.5699540244238, accuracy: 0.7359154929577465\n",
      "iter: 74, loss: 580.4950661024482, accuracy: 0.40492957746478875\n",
      "iter: 75, loss: 419.97987416247355, accuracy: 0.7288732394366197\n",
      "iter: 76, loss: 657.7645782724686, accuracy: 0.40492957746478875\n",
      "iter: 77, loss: 369.21394256443847, accuracy: 0.7359154929577465\n",
      "iter: 78, loss: 580.5765112271517, accuracy: 0.40492957746478875\n",
      "iter: 79, loss: 406.8887347598932, accuracy: 0.7323943661971831\n",
      "iter: 80, loss: 618.4876577117672, accuracy: 0.40492957746478875\n",
      "iter: 81, loss: 380.6564433927188, accuracy: 0.7359154929577465\n",
      "iter: 82, loss: 574.1009608871223, accuracy: 0.40492957746478875\n",
      "iter: 83, loss: 397.73549997974806, accuracy: 0.7359154929577465\n",
      "iter: 84, loss: 584.9833043935838, accuracy: 0.40492957746478875\n",
      "iter: 85, loss: 386.6250621918022, accuracy: 0.7359154929577465\n",
      "iter: 86, loss: 560.5971712256692, accuracy: 0.40492957746478875\n",
      "iter: 87, loss: 392.133289785828, accuracy: 0.7359154929577465\n",
      "iter: 88, loss: 557.1179632324177, accuracy: 0.40492957746478875\n",
      "iter: 89, loss: 387.95340927874133, accuracy: 0.7359154929577465\n",
      "iter: 90, loss: 542.0297833539086, accuracy: 0.40492957746478875\n",
      "iter: 91, loss: 388.42103241065104, accuracy: 0.7359154929577465\n",
      "iter: 92, loss: 532.88153046292, accuracy: 0.4084507042253521\n",
      "iter: 93, loss: 386.3030260940092, accuracy: 0.7359154929577465\n",
      "iter: 94, loss: 520.9640664361867, accuracy: 0.4119718309859155\n",
      "iter: 95, loss: 384.92299172088656, accuracy: 0.7359154929577465\n",
      "iter: 96, loss: 510.2573725675242, accuracy: 0.4119718309859155\n",
      "iter: 97, loss: 382.8767322293339, accuracy: 0.7359154929577465\n",
      "iter: 98, loss: 499.1270692383419, accuracy: 0.4154929577464789\n",
      "iter: 99, loss: 380.6814432995435, accuracy: 0.7359154929577465\n",
      "iter: 100, loss: 488.1892852209644, accuracy: 0.4154929577464789\n",
      "iter: 101, loss: 378.15611335655706, accuracy: 0.7359154929577465\n",
      "iter: 102, loss: 477.24960884548983, accuracy: 0.4154929577464789\n",
      "iter: 103, loss: 375.34703803746504, accuracy: 0.7359154929577465\n",
      "iter: 104, loss: 466.3703604234455, accuracy: 0.4154929577464789\n",
      "iter: 105, loss: 372.2387115702601, accuracy: 0.7464788732394366\n",
      "iter: 106, loss: 455.5406074213531, accuracy: 0.4154929577464789\n",
      "iter: 107, loss: 368.832359235363, accuracy: 0.7464788732394366\n",
      "iter: 108, loss: 444.76452724144656, accuracy: 0.4154929577464789\n",
      "iter: 109, loss: 365.127425105269, accuracy: 0.7464788732394366\n",
      "iter: 110, loss: 434.0411960570612, accuracy: 0.41901408450704225\n",
      "iter: 111, loss: 361.12472600774055, accuracy: 0.7570422535211268\n",
      "iter: 112, loss: 423.3689891279819, accuracy: 0.4225352112676056\n",
      "iter: 113, loss: 356.825562518191, accuracy: 0.7570422535211268\n",
      "iter: 114, loss: 412.7459966427455, accuracy: 0.4295774647887324\n",
      "iter: 115, loss: 352.23156512315745, accuracy: 0.7605633802816901\n",
      "iter: 116, loss: 402.1715259094932, accuracy: 0.43309859154929575\n",
      "iter: 117, loss: 347.34475058637526, accuracy: 0.7640845070422535\n",
      "iter: 118, loss: 391.6478510417429, accuracy: 0.44366197183098594\n",
      "iter: 119, loss: 342.1679827761651, accuracy: 0.7640845070422535\n",
      "iter: 120, loss: 381.1821021333743, accuracy: 0.44366197183098594\n",
      "iter: 121, loss: 336.70590624041773, accuracy: 0.7640845070422535\n",
      "iter: 122, loss: 370.78803756548473, accuracy: 0.4471830985915493\n",
      "iter: 123, loss: 330.96634452194405, accuracy: 0.7711267605633803\n",
      "iter: 124, loss: 360.4873850483615, accuracy: 0.4471830985915493\n",
      "iter: 125, loss: 324.9620406868224, accuracy: 0.7711267605633803\n",
      "iter: 126, loss: 350.3103793508139, accuracy: 0.4612676056338028\n",
      "iter: 127, loss: 318.71247173001154, accuracy: 0.7711267605633803\n",
      "iter: 128, loss: 340.29509800958397, accuracy: 0.4612676056338028\n",
      "iter: 129, loss: 312.24530742582317, accuracy: 0.7711267605633803\n",
      "iter: 130, loss: 330.48524805775355, accuracy: 0.4753521126760563\n",
      "iter: 131, loss: 305.5969534869667, accuracy: 0.7816901408450704\n",
      "iter: 132, loss: 320.92624152705366, accuracy: 0.4753521126760563\n",
      "iter: 133, loss: 298.81159705360267, accuracy: 0.7816901408450704\n",
      "iter: 134, loss: 311.65973812395066, accuracy: 0.4823943661971831\n",
      "iter: 135, loss: 291.9383466368174, accuracy: 0.7887323943661971\n",
      "iter: 136, loss: 302.7172726389676, accuracy: 0.4894366197183099\n",
      "iter: 137, loss: 285.0264620879614, accuracy: 0.7887323943661971\n",
      "iter: 138, loss: 294.1139587117965, accuracy: 0.5\n",
      "iter: 139, loss: 278.1192108425207, accuracy: 0.7887323943661971\n",
      "iter: 140, loss: 285.84334030233396, accuracy: 0.5035211267605634\n",
      "iter: 141, loss: 271.2473264741038, accuracy: 0.7887323943661971\n",
      "iter: 142, loss: 277.8740765816867, accuracy: 0.5070422535211268\n",
      "iter: 143, loss: 264.4230958581558, accuracy: 0.7922535211267606\n",
      "iter: 144, loss: 270.14832620757403, accuracy: 0.5246478873239436\n",
      "iter: 145, loss: 257.6356085249129, accuracy: 0.7922535211267606\n",
      "iter: 146, loss: 262.58069795638727, accuracy: 0.5387323943661971\n",
      "iter: 147, loss: 250.84677501914686, accuracy: 0.8028169014084507\n",
      "iter: 148, loss: 255.0557460917265, accuracy: 0.5563380281690141\n",
      "iter: 149, loss: 243.98663633550683, accuracy: 0.8063380281690141\n",
      "iter: 150, loss: 247.421262842731, accuracy: 0.5669014084507042\n",
      "iter: 151, loss: 236.94541185528365, accuracy: 0.8063380281690141\n",
      "iter: 152, loss: 239.47379129094335, accuracy: 0.5809859154929577\n",
      "iter: 153, loss: 229.55854580751608, accuracy: 0.8133802816901409\n",
      "iter: 154, loss: 230.93167286916048, accuracy: 0.5985915492957746\n",
      "iter: 155, loss: 221.57960659637877, accuracy: 0.823943661971831\n",
      "iter: 156, loss: 221.39136701658492, accuracy: 0.6232394366197183\n",
      "iter: 157, loss: 212.63614675676433, accuracy: 0.8274647887323944\n",
      "iter: 158, loss: 210.27717117453645, accuracy: 0.6408450704225352\n",
      "iter: 159, loss: 202.1785979208012, accuracy: 0.8345070422535211\n",
      "iter: 160, loss: 196.87884200444307, accuracy: 0.6514084507042254\n",
      "iter: 161, loss: 189.5204499223498, accuracy: 0.8415492957746479\n",
      "iter: 162, loss: 180.85273896188414, accuracy: 0.6725352112676056\n",
      "iter: 163, loss: 174.3553389738586, accuracy: 0.8556338028169014\n",
      "iter: 164, loss: 163.55074719886613, accuracy: 0.6866197183098591\n",
      "iter: 165, loss: 158.09124877566535, accuracy: 0.852112676056338\n",
      "iter: 166, loss: 147.98621614728293, accuracy: 0.7147887323943662\n",
      "iter: 167, loss: 143.7004094934807, accuracy: 0.8661971830985915\n",
      "iter: 168, loss: 135.85976869199655, accuracy: 0.7288732394366197\n",
      "iter: 169, loss: 132.69146563770596, accuracy: 0.8873239436619719\n",
      "iter: 170, loss: 126.95541398661649, accuracy: 0.7570422535211268\n",
      "iter: 171, loss: 124.70184706504514, accuracy: 0.8908450704225352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 172, loss: 120.51624127691312, accuracy: 0.7570422535211268\n",
      "iter: 173, loss: 118.94092319533793, accuracy: 0.8838028169014085\n",
      "iter: 174, loss: 115.85678081644602, accuracy: 0.778169014084507\n",
      "iter: 175, loss: 114.75391157088734, accuracy: 0.8802816901408451\n",
      "iter: 176, loss: 112.4522149787882, accuracy: 0.7816901408450704\n",
      "iter: 177, loss: 111.6674783397111, accuracy: 0.8802816901408451\n",
      "iter: 178, loss: 109.922512659951, accuracy: 0.7922535211267606\n",
      "iter: 179, loss: 109.34969951642097, accuracy: 0.8767605633802817\n",
      "iter: 180, loss: 108.00247899556788, accuracy: 0.8028169014084507\n",
      "iter: 181, loss: 107.57162252958248, accuracy: 0.8661971830985915\n",
      "iter: 182, loss: 106.51123094202381, accuracy: 0.8063380281690141\n",
      "iter: 183, loss: 106.1769421267781, accuracy: 0.8661971830985915\n",
      "iter: 184, loss: 105.32639768886432, accuracy: 0.8098591549295775\n",
      "iter: 185, loss: 105.05916099254028, accuracy: 0.8661971830985915\n",
      "iter: 186, loss: 104.36492010732817, accuracy: 0.8169014084507042\n",
      "iter: 187, loss: 104.14530046559545, accuracy: 0.8697183098591549\n",
      "iter: 188, loss: 103.56981720759437, accuracy: 0.8169014084507042\n",
      "iter: 189, loss: 103.3847886039844, accuracy: 0.8697183098591549\n",
      "iter: 190, loss: 102.90141953682132, accuracy: 0.8169014084507042\n",
      "iter: 191, loss: 102.7420757253847, accuracy: 0.8697183098591549\n",
      "iter: 192, loss: 102.33165201363984, accuracy: 0.8169014084507042\n",
      "iter: 193, loss: 102.19177775377979, accuracy: 0.8697183098591549\n",
      "iter: 194, loss: 101.84030986337171, accuracy: 0.8204225352112676\n",
      "iter: 195, loss: 101.71548034493564, accuracy: 0.8697183098591549\n",
      "iter: 196, loss: 101.41261529641477, accuracy: 0.8204225352112676\n",
      "iter: 197, loss: 101.29962033248205, accuracy: 0.8697183098591549\n",
      "iter: 198, loss: 101.03759546047083, accuracy: 0.8204225352112676\n",
      "iter: 199, loss: 100.93406539998665, accuracy: 0.8697183098591549\n",
      "iter: 200, loss: 100.70699037084383, accuracy: 0.8204225352112676\n",
      "iter: 201, loss: 100.61114913003773, accuracy: 0.8661971830985915\n",
      "iter: 202, loss: 100.41450661735146, accuracy: 0.8204225352112676\n",
      "iter: 203, loss: 100.32500615885263, accuracy: 0.8661971830985915\n",
      "iter: 204, loss: 100.155299719107, accuracy: 0.8204225352112676\n",
      "iter: 205, loss: 100.07110763693566, accuracy: 0.8661971830985915\n",
      "iter: 206, loss: 99.9256098948454, accuracy: 0.8204225352112676\n",
      "iter: 207, loss: 99.84593224518326, accuracy: 0.8661971830985915\n",
      "iter: 208, loss: 99.72250232009414, accuracy: 0.8204225352112676\n",
      "iter: 209, loss: 99.64673026220282, accuracy: 0.8661971830985915\n",
      "iter: 210, loss: 99.5436796127249, accuracy: 0.8204225352112676\n",
      "iter: 211, loss: 99.4713524218492, accuracy: 0.8661971830985915\n",
      "iter: 212, loss: 99.38734497864141, accuracy: 0.8204225352112676\n",
      "iter: 213, loss: 99.3181245201119, accuracy: 0.8661971830985915\n",
      "iter: 214, loss: 99.2521013945397, accuracy: 0.8204225352112676\n",
      "iter: 215, loss: 99.18575477260656, accuracy: 0.8661971830985915\n",
      "iter: 216, loss: 99.1368767782964, accuracy: 0.8204225352112676\n",
      "iter: 217, loss: 99.07326493471383, accuracy: 0.8661971830985915\n",
      "iter: 218, loss: 99.04086815083723, accuracy: 0.8204225352112676\n",
      "iter: 219, loss: 98.97993889377354, accuracy: 0.8661971830985915\n",
      "iter: 220, loss: 98.96349985942228, accuracy: 0.8204225352112676\n",
      "iter: 221, loss: 98.90528428058569, accuracy: 0.8661971830985915\n",
      "iter: 222, loss: 98.90439234895062, accuracy: 0.8204225352112676\n",
      "* 0.0 0.8210526315789474 0.8204225352112676\n"
     ]
    }
   ],
   "source": [
    "from random import uniform\n",
    "\n",
    "def neg_log_likelihood(X, w, y):\n",
    "    '''Compute the negative log likelihood'''\n",
    "    L = 0\n",
    "    for _x,_y in zip(X, y):\n",
    "        z = w.dot(_x)\n",
    "        L += log_prob(z=z, y_i=_y)\n",
    "    return -1 * L\n",
    "\n",
    "\n",
    "def fast_logistic(X, w):\n",
    "    '''Compute the logistic function over many data points'''\n",
    "    return 1/(1 + np.exp(-1 * X.dot(w)))\n",
    "\n",
    "\n",
    "def grad(X, w, y, lambda_=.5):\n",
    "    '''\n",
    "    Return the gradient\n",
    "    \n",
    "    - https://web.stanford.edu/~jurafsky/slp3/5.pdf\n",
    "    '''\n",
    "    grad = np.zeros_like(w)\n",
    "    batch_size, F = X.shape\n",
    "    \n",
    "    b = X * (fast_logistic(X, w) - y).reshape((batch_size, 1))\n",
    "\n",
    "    return np.sum(b, axis=0) + (lambda_ * 2 * w)\n",
    "\n",
    "\n",
    "def squared_l2_norm(x):\n",
    "    '''\n",
    "    $\\sqrt{\\Sigma x_i^2} ^ 2\n",
    "    '''\n",
    "    return np.sum(x ** 2)\n",
    "\n",
    "\n",
    "def grad_decent(_X, _y, eta = .0001, lambda_ = 0, tolerance=1e-4, verbose=True, batch_size=None):\n",
    "    '''\n",
    "    - Perform gradient ascent\n",
    "    - This function is basically the same as in the Adeline notebook\n",
    "    - Of course, the gradient is different, because it is a different function\n",
    "    '''\n",
    "    N, dim = _X.shape\n",
    "    w = np.random.uniform(low=-.1, high=.1, size=dim)\n",
    "    last = 0\n",
    "    for i in range(1000):\n",
    "        this_ll = neg_log_likelihood(_X, w, _y)\n",
    "        loss = this_ll + lambda_ * squared_l2_norm(w)\n",
    "        if verbose:\n",
    "            print(\"iter: {}, loss: {}, accuracy: {}\".format(i, loss, accuracy(_X, w, _y)))\n",
    "        if(abs(this_ll - last) < tolerance): break\n",
    "        last = this_ll\n",
    "        if batch_size is not None:\n",
    "            w -= eta * grad(_X, w, _y, lambda_=lambda_)\n",
    "        else:\n",
    "            _N,F = _X.shape\n",
    "            idx = np.random.randint(_N, size=batch_size)\n",
    "            w -= eta * grad(_X, w, _y, lambda_=lambda_)\n",
    "    return w\n",
    "\n",
    "def prediction(X, w, threshold=.5):\n",
    "    '''\n",
    "    - Return a Boolean array of length N.\n",
    "    - The array should be True if the weights dotted with the features for a given instance is greater than .5\n",
    "    '''\n",
    "    N, D = X.shape\n",
    "    return X.dot(w) > threshold\n",
    "\n",
    "def accuracy(X, w, y):\n",
    "    '''\n",
    "    Return a value between 0 and 1, showing the fraction of data points which have been classified correctly\n",
    "    '''\n",
    "    return np.mean(prediction(X, w) == y)\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = datasets.load_breast_cancer()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "N, dim = X.shape\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "for lambda_ in [0.]:\n",
    "    w = grad_decent(X_train, y_train, eta=.0000001, tolerance=.001, verbose=True, lambda_=lambda_, batch_size=None)\n",
    "    test_accuracy = accuracy(X_test, w, y_test)\n",
    "    train_accuracy = accuracy(X_train, w, y_train)\n",
    "    print('*', lambda_, test_accuracy, train_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Norm questions\n",
    "\n",
    "- Complete the squared L2 norm function\n",
    "- What does \\lambda do? \n",
    "- What happens if you set lambda = 5000?\n",
    "- Try computing the norm of w with different lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad(X, w, y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
