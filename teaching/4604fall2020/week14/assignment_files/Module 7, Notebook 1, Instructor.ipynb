{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with unstructured data (Module 07)\n",
    "\n",
    "\n",
    "- A huge area. We will just scratch the surface.\n",
    "- We will focus on text, but this also applies to images, audio data, some sensor data ...  \n",
    "- I first heard about this area ~7 years ago and was very excited. I think it is a little more common these days? \n",
    "- Why text? People interact with each other through language, so computational text analysis is a neat way to combine interests in computers & human society.\n",
    "- Jargon: \"natural language processing,\" \"computational linguistics,\" \"text as data,\" \"NLP+CSS.\" These things have slightly different meanings, but there is a lot of overlap.\n",
    "\n",
    "#### Resources\n",
    "\n",
    "If six 50-minute sessions working with text data is not enough for you, there are many ways to keep going with it.\n",
    "\n",
    "- Courses:\n",
    "    - [LING 1200](https://catalog.colorado.edu/courses-a-z/ling/) (cross listed with INFO)\n",
    "    - [MS degree at CU](https://www.colorado.edu/linguistics/graduate-program/computational-linguistics-clasic-ms)\n",
    "\n",
    "\n",
    "- Books: \n",
    "    - [Intro to Natural Language Processing](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf) (Available as hardback, or use the linked PDF off GitHub.)\n",
    "    - [Foundations of Statistical Natural Language Processing](https://nlp.stanford.edu/fsnlp/) (Older text book, still good. You can find PDFs online.)\n",
    "    \n",
    "    \n",
    "- Conferences:\n",
    "    - [Text as data](https://www.textasdata2019.net/) (Political science + computer science)\n",
    "    - [CS + J](http://cplusj.org/) (Not strictly for text, but comes up a lot)\n",
    "    - [*CL](http://aclweb.org/) (Umbrella org for conferences focused on computers + text, aka \"natural langauge processing\")\n",
    "    - [#NLProc](https://twitter.com/hashtag/nlproc?lang=en) (A group of very knowledgeable researchers and practitioners talk about this stuff all day on Twitter. If you want to keep up with the latest and greatest, this is a good way to do so.)\n",
    "    \n",
    "\n",
    "- Software:\n",
    "    - [NLTK](https://github.com/nltk/nltk) Very popular software for NLP in Python. Common entry into NLP.\n",
    "    - [Spacy](https://spacy.io/) Another popular Python NLP library. Way more performant than NLTK, and perhaps better maintained. \n",
    "    - [Hugging Face](https://github.com/huggingface) New NLP library focused on specific kinds of neural networks that are very popular. You might find this one hard to work with, but good to know about.\n",
    "    - [AllenNLP](https://allennlp.org/) Another new-ish one that is good to just be aware of. More focused on research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured vs. unstructured data\n",
    "\n",
    "Examples:\n",
    "- [Structured](https://github.com/nytimes/covid-19-data/blob/master/us.csv)\n",
    "- [Unstructured](https://www.reddit.com/r/cuboulder/)\n",
    "\n",
    "Questions:\n",
    "- What is the difference between structured and unstructured?\n",
    "- What are other examples of unstructured data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text step 1: Tokenization \n",
    "\n",
    "- Text usually starts as a string of characters\n",
    "- We need to take that string of characters and turn it into a list of words\n",
    "- Tokenization is the process of doing so\n",
    "- _Sorry, 4604 students. Everyone needs to know this. It is the first step in most NLP pipelines. There is a little overlap here w/ recent 4604 classes._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'ca', \"n't\", 'go', 'outside']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ptb import TreebankWordTokenizer\n",
    "# This is the Penn Tree Bank tokenizer from NLTK as just one file\n",
    "tok = TreebankWordTokenizer()\n",
    "tok.tokenize(\"Hello this is a tokenizer\")\n",
    "\n",
    "tok.tokenize(\"I can't go outside\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions: \n",
    "    \n",
    "- How is this different than split (h/t Jay) \n",
    "- Why do you think it might make sense to split \"can't\" into \"ca,\" \"n't\"? What information does this give to a computer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions: \n",
    "    \n",
    "- Read in `dogs10k.jsonl`. Access the `body` field. This is cleaned up data from the pushift API\n",
    "\n",
    "- Why is this unstructured data?\n",
    "\n",
    "- Can you figure out how to find the post from the data on reddit? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Questions: \n",
    "    \n",
    "- Tokenize the body of each post on `dogs10k.jsonl` to create a vocabulary set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "V = set()\n",
    "D = 0\n",
    "\n",
    "with open(\"dogs10k.jsonl\", \"r\") as inf:\n",
    "    for doc in inf:\n",
    "        D += 1\n",
    "        doc = json.loads(doc)\n",
    "        for token in tok.tokenize(doc[\"body\"]):\n",
    "            V.add(token)\n",
    "            \n",
    "V = list(V) # we want a consistent order. Not sure the latest on Python set ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Questions: \n",
    "    \n",
    "- What is D?\n",
    "- What is V?\n",
    "- Is D bigger than V?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 A\n",
      "1 B\n",
      "2 C\n"
     ]
    }
   ],
   "source": [
    "# understanding enumerate\n",
    "\n",
    "for docno, doc in enumerate([\"A\", \"B\", \"C\"]):\n",
    "    print(docno, doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numberization\n",
    "\n",
    "- A great word\n",
    "- Turning words into numbers\n",
    "- This is honestly a mind-bender\n",
    "- Pause for a minute here. We are going from words to numbers.\n",
    "- When we use numbers, we can use tools from math to reason about words. \n",
    "- Pause. Breathe.\n",
    "- [Cool book](https://web.stanford.edu/group/cslipublications/cslipublications/site/1575864487.shtml) about this. It's a big conceptual leap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping words to numbers and back\n",
    "\n",
    "for docno, doc in enumerate([\"A\", \"B\", \"C\"]):\n",
    "    print(docno, doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the mapping\n",
    "\n",
    "v2n = {}\n",
    "n2v = {}\n",
    "for vno, v in enumerate(V):\n",
    "    v2n[v] = vno\n",
    "    n2v[vno] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions \n",
    "\n",
    "- What is n2v for \"fetch\"\n",
    "- What is v2n for 15?\n",
    "- How many times does each word occur in $V$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a binary term document matrix\n",
    "# The rows are documents, ths columns are words \n",
    "# The column is set to 1 if the word occurs in the document\n",
    "\n",
    "tdm = np.zeros((D, len(V)))\n",
    "\n",
    "with open(\"dogs10k.jsonl\", \"r\") as inf:\n",
    "    for docno, doc in enumerate(inf):\n",
    "        D += 1\n",
    "        doc = json.loads(doc)\n",
    "        for token in tok.tokenize(doc[\"body\"]):\n",
    "            tdm[docno][v2n[token]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "obsession                    0.0\n",
       "preferably                   0.0\n",
       "//i.imgur.com/kJKPkLk.jpg    0.0\n",
       "picture                      0.0\n",
       "passed                       0.0\n",
       "                            ... \n",
       "drowning                     0.0\n",
       "migrate                      0.0\n",
       "hustle                       0.0\n",
       "meat.                        0.0\n",
       "pleaser                      0.0\n",
       "Name: 0, Length: 29829, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tdm_df = pd.DataFrame(data=tdm, columns=V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparsity \n",
    "\n",
    "- What do you notice about each row of `tdm_df`?  # hint: remember our friend iloc\n",
    "\n",
    "- What do you notice about each column of `tdm_df`?\n",
    "\n",
    "- Why does this make sense?\n",
    "\n",
    "- This is a *fundamental* property of text data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Discrete vs. continuous \n",
    "\n",
    "- Are these rows and columns discrete or continuous? \n",
    "- This is a *fundamental* property of text data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Review\n",
    "\n",
    "- What are two fundamental properties of text data?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
