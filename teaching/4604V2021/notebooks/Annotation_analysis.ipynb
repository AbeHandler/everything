{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation analysis\n",
    "\n",
    "In this notebook, we will analyze the data we annotated as a class. ML often relies on human-annotated data. It is very, very important to check if humans actually agree on labels _before_ you start designing a model. If people don't agree then the machine has no hope! \n",
    "\n",
    "This is a very, very common mistake in applied ML. It is imperative to validate that you even have a well-defined task before you start predicting things. \n",
    "\n",
    "In this notebook, we will analyze the extent to which different annotators agreed during annotation on Friday. If the annotators don't agree with enough regularity, it does not make sense to proceed to modeling. You have a BS task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Take aways\n",
    "\n",
    "1. Do annotators actually agree? \n",
    "2. To measure this we compute a pairwise agreement statistic\n",
    "3. We also need to ask: do they actually agree at rates higher than chance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "\n",
    "def get_results(input_directory = \"submissions/\", output_file = \"class.jsonl\"):\n",
    "    '''\n",
    "    In this function we will manipulate raw data to make a list of dictionaries.\n",
    "\n",
    "    In this case, we want a list of dictionaries which each has the following fields:\n",
    "        - text holds the text that was judged\n",
    "        - text_id holds and ID for the text\n",
    "        - label holds the annotator's label\n",
    "        - annotator holds the annotator's name\n",
    "    \n",
    "    '''\n",
    "    out = []\n",
    "\n",
    "    for submission in glob(input_directory + \"*\"):\n",
    "        category = \"TODO\" # should be yelp or emotion\n",
    "        annotator = \"TODO\"\n",
    "        with open(submission, \"r\") as inf:\n",
    "            fl = csv.reader(inf)\n",
    "            # your code here\n",
    "\n",
    "    return out\n",
    "            \n",
    "all_results = get_results(input_directory = \"submissions/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some basic questions\n",
    "- How many annotators are there? \n",
    "\n",
    "[your answer here]\n",
    "\n",
    "- How many total reviews are there? \n",
    "\n",
    "[your answer here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotators(results):\n",
    "    '''\n",
    "    Get a set of all annotators in the dataset\n",
    "    '''\n",
    "    annotators = set()\n",
    "    # your code here\n",
    "    return annotators\n",
    "\n",
    "def get_reviews2judgments(results):\n",
    "    '''\n",
    "    Return a map from a given text_id to all judgments for that text_id\n",
    "    '''\n",
    "    review2judgments = defaultdict(list)\n",
    "\n",
    "    # your code here\n",
    "\n",
    "    return review2judgments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More data exploration\n",
    "\n",
    "- Do annotators tend to agree on item \\#2? Does that make sense to you?\n",
    "\n",
    "[your answer here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4899691806710473"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pairwise_agreement(results):\n",
    "    '''\n",
    "    Compute the pairwise agreement between raters for the input results\n",
    "    \n",
    "    To compute pairwise agreement compare judgements from all pairs of annotators\n",
    "    Return the fraction of pairs of annotators who agree\n",
    "    '''\n",
    "    total = 0\n",
    "    agrees = 0\n",
    "    for result in results:\n",
    "        for other_result in results:\n",
    "            # count how many times the annotators agree\n",
    "            # be sure to only count agreement for instances in the same category, and when there are different \n",
    "            # annotators. Be sure to skip over cases where only one annotator applied a label\n",
    "            # also be sure to only count agreements for the same unit of text\n",
    "    return agrees/total\n",
    "\n",
    "pairwise_agreement([k for k in all_results if k[\"category\"] == \"yelp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Per-category analysis\n",
    "\n",
    "- Does Yelp or emotion data have higher or lower pairwise agreement? Does that make sense?\n",
    "\n",
    "[Type your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per-item analysis\n",
    "\n",
    "- Which review has the highest and lowest pairwise agreement rate? Does this make sense?\n",
    "\n",
    "[Type your answer here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4959"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import random\n",
    "\n",
    "def annotator1():\n",
    "    return random() < .5\n",
    "\n",
    "def annotator2():\n",
    "    return random() < .5\n",
    "\n",
    "trials = 10000\n",
    "agreements = 0\n",
    "for j in range(trials):\n",
    "    if annotator1() == annotator2():\n",
    "        agreements += 1\n",
    "        \n",
    "agreements/trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random agreement rate\n",
    "\n",
    "If two reviewers answered randomly (meaning just picked random annotations) how often would they agree just by chance?\n",
    "\n",
    "[Type your answer here, and explain your reasoning]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fleiss Kappa\n",
    "\n",
    "[Fleiss kappa](https://en.wikipedia.org/wiki/Fleiss%27_kappa) measures the exent to which pairs of reviewers agree, as compared to how much they would agree by chance. \n",
    "\n",
    "- $\\bar{P}_e$ is the rate at which reviewers agree by chance \n",
    "- $\\bar{P}$ is the pairwise agreement rate across all items the dataset\n",
    "    - note: the Wikipedia article uses a slightly different definition of $\\bar{P}$, because it assumes all reviewers review all items, which is not true in our case\n",
    "\n",
    "\n",
    "$\\kappa = \\frac{\\bar{P} - \\bar{P}_e}{1-\\bar{P}_e}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the highest possible value of Fleiss Kappa? What is the lowest?\n",
    "\n",
    "[Type your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What does the denominator mean? If $\\bar{P}_e$ is high, then is the denominator high or low?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If $\\bar{P}$ is high and $\\bar{P_e}$ is high, do you think the task is well-defined?\n",
    "\n",
    "[Type your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If $\\bar{P}$ is low and $\\bar{P_e}$ is high, do you think the task is well-defined\n",
    "\n",
    "[Type your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If $\\bar{P}$ is high and $\\bar{P_e}$ is low, do you think the task is well-defined?\n",
    "\n",
    "[Type your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What do you think the Fleiss Kappa will be for the Yelp data set? Do you think it will be higher or lower than for the emotions dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Fleiss Kappa for the dataset\n",
    "\n",
    "def kappa(Pe, Pbar):\n",
    "    return 0\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
