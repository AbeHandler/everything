{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dmse(xs, ys, m):\n",
    "    '''derivative of mse function (in 1-D)'''\n",
    "    sum_ = 0\n",
    "    for _x, _y in zip(xs, ys):\n",
    "        sum_ += -2 * _x * (_y - (m * _x))\n",
    "    return sum_/len(xs)\n",
    "\n",
    "def mse(preds, ys):\n",
    "    '''\n",
    "    mean squared error function. \n",
    "    preds are predictions, ys are truth\n",
    "    '''\n",
    "    sq = 0 \n",
    "    for p, y in zip(preds, ys):\n",
    "        sq += (p - y) ** 2\n",
    "    return sq/len(ys)\n",
    "\n",
    "def get_preds(m_t, xs):\n",
    "    '''\n",
    "    Get predictions for xs, based on the parameter m_t\n",
    "    '''\n",
    "    preds = []\n",
    "    for _x in xs:\n",
    "        p = m_t * _x\n",
    "        preds.append(p)\n",
    "    return preds\n",
    "\n",
    "def generate_some_data(real_m=2):\n",
    "    '''\n",
    "    Generate some data, using the equation y=mx + e \n",
    "    where e is random Gaussian error\n",
    "    '''\n",
    "    xs = np.linspace(0, 10)\n",
    "    ys = []\n",
    "    for _x in xs:\n",
    "        e = np.random.normal(0, 1, 1)[0]\n",
    "        y = (_x * real_m) + e\n",
    "        ys.append(y)\n",
    "    return ys\n",
    "\n",
    "def plot_line(xs, ys, m_t):\n",
    "    data = []\n",
    "    for x, y in zip(xs, ys):\n",
    "        pred = m_t * x\n",
    "        data.append({\"x\": x, \"y\": y, \"pred\": pred})\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    c = alt.Chart(df).mark_point().encode(\n",
    "        x=\"x\",\n",
    "        y=\"y\"\n",
    "    )\n",
    "\n",
    "    d = alt.Chart(df).mark_line().encode(\n",
    "        x=\"x\",\n",
    "        y=\"pred\"\n",
    "    )\n",
    "\n",
    "    return c + d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-6b58d408012a4e248fefdd27e14668dd\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-6b58d408012a4e248fefdd27e14668dd\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-6b58d408012a4e248fefdd27e14668dd\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"layer\": [{\"mark\": \"point\", \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"x\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"y\"}}}, {\"mark\": \"line\", \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"x\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"pred\"}}}], \"data\": {\"name\": \"data-9b7dbd0a2ff4a09d56cec0ea87474a5f\"}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-9b7dbd0a2ff4a09d56cec0ea87474a5f\": [{\"x\": 0.0, \"y\": -0.4933706600056078, \"pred\": 0.0}, {\"x\": 0.20408163265306123, \"y\": -0.806840209108193, \"pred\": 0.3069013463715249}, {\"x\": 0.40816326530612246, \"y\": -0.6950011253584472, \"pred\": 0.6138026927430498}, {\"x\": 0.6122448979591837, \"y\": 0.3319699549369811, \"pred\": 0.9207040391145745}, {\"x\": 0.8163265306122449, \"y\": 2.841216438328535, \"pred\": 1.2276053854860995}, {\"x\": 1.0204081632653061, \"y\": 3.249162869355073, \"pred\": 1.5345067318576242}, {\"x\": 1.2244897959183674, \"y\": 2.8005731937453513, \"pred\": 1.841408078229149}, {\"x\": 1.4285714285714286, \"y\": 3.1454499012124257, \"pred\": 2.148309424600674}, {\"x\": 1.6326530612244898, \"y\": 2.5840617545380025, \"pred\": 2.455210770972199}, {\"x\": 1.836734693877551, \"y\": 4.0418776114054715, \"pred\": 2.7621121173437237}, {\"x\": 2.0408163265306123, \"y\": 4.9828789115410475, \"pred\": 3.0690134637152484}, {\"x\": 2.2448979591836737, \"y\": 5.085048872955781, \"pred\": 3.375914810086774}, {\"x\": 2.4489795918367347, \"y\": 6.051352827758742, \"pred\": 3.682816156458298}, {\"x\": 2.6530612244897958, \"y\": 5.29960467155291, \"pred\": 3.989717502829823}, {\"x\": 2.857142857142857, \"y\": 5.7820564454666, \"pred\": 4.296618849201348}, {\"x\": 3.0612244897959187, \"y\": 7.323313054677241, \"pred\": 4.603520195572873}, {\"x\": 3.2653061224489797, \"y\": 5.355154114736505, \"pred\": 4.910421541944398}, {\"x\": 3.4693877551020407, \"y\": 5.666256262457428, \"pred\": 5.217322888315922}, {\"x\": 3.673469387755102, \"y\": 7.522328193790268, \"pred\": 5.5242242346874475}, {\"x\": 3.8775510204081636, \"y\": 9.397862838488672, \"pred\": 5.831125581058973}, {\"x\": 4.081632653061225, \"y\": 10.647938215291466, \"pred\": 6.138026927430497}, {\"x\": 4.285714285714286, \"y\": 9.47969993769306, \"pred\": 6.444928273802022}, {\"x\": 4.4897959183673475, \"y\": 9.944591307467494, \"pred\": 6.751829620173548}, {\"x\": 4.6938775510204085, \"y\": 9.699515310592883, \"pred\": 7.058730966545072}, {\"x\": 4.8979591836734695, \"y\": 10.753335892669627, \"pred\": 7.365632312916596}, {\"x\": 5.1020408163265305, \"y\": 10.305601098882669, \"pred\": 7.6725336592881215}, {\"x\": 5.3061224489795915, \"y\": 8.7528985360059, \"pred\": 7.979435005659646}, {\"x\": 5.510204081632653, \"y\": 11.593899640296419, \"pred\": 8.28633635203117}, {\"x\": 5.714285714285714, \"y\": 11.140133766775065, \"pred\": 8.593237698402696}, {\"x\": 5.918367346938775, \"y\": 10.573933263872163, \"pred\": 8.900139044774221}, {\"x\": 6.122448979591837, \"y\": 11.514933778328121, \"pred\": 9.207040391145746}, {\"x\": 6.326530612244898, \"y\": 13.461409201469339, \"pred\": 9.513941737517271}, {\"x\": 6.530612244897959, \"y\": 12.566707249876181, \"pred\": 9.820843083888796}, {\"x\": 6.73469387755102, \"y\": 11.297578134897089, \"pred\": 10.12774443026032}, {\"x\": 6.938775510204081, \"y\": 13.145766213924928, \"pred\": 10.434645776631845}, {\"x\": 7.142857142857143, \"y\": 13.676160263419327, \"pred\": 10.74154712300337}, {\"x\": 7.346938775510204, \"y\": 14.393595827502885, \"pred\": 11.048448469374895}, {\"x\": 7.551020408163265, \"y\": 14.911038691873525, \"pred\": 11.35534981574642}, {\"x\": 7.755102040816327, \"y\": 16.790484661489728, \"pred\": 11.662251162117945}, {\"x\": 7.959183673469388, \"y\": 15.106941560528774, \"pred\": 11.96915250848947}, {\"x\": 8.16326530612245, \"y\": 16.474837745706232, \"pred\": 12.276053854860994}, {\"x\": 8.36734693877551, \"y\": 15.827741449957148, \"pred\": 12.582955201232519}, {\"x\": 8.571428571428571, \"y\": 18.145612502091367, \"pred\": 12.889856547604044}, {\"x\": 8.775510204081632, \"y\": 18.724685571039657, \"pred\": 13.196757893975567}, {\"x\": 8.979591836734695, \"y\": 17.690146533204334, \"pred\": 13.503659240347096}, {\"x\": 9.183673469387756, \"y\": 18.206282948062213, \"pred\": 13.81056058671862}, {\"x\": 9.387755102040817, \"y\": 18.80291905468071, \"pred\": 14.117461933090144}, {\"x\": 9.591836734693878, \"y\": 19.006837738163618, \"pred\": 14.42436327946167}, {\"x\": 9.795918367346939, \"y\": 19.719371102369234, \"pred\": 14.731264625833193}, {\"x\": 10.0, \"y\": 20.533443802895157, \"pred\": 15.038165972204718}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learning via gradient descent\n",
    "eta = .001 \n",
    "m_t = 1\n",
    "iters = 10\n",
    "\n",
    "for i in range(iters):\n",
    "    d = dmse(xs, ys, m_t)\n",
    "    m_t -= d * eta\n",
    "    preds = get_preds(m_t, xs)\n",
    "    c = plot_line(xs, ys, m_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions \n",
    "\n",
    "1. What is each line in the cell called \"learning via gradient descent\" doing?\n",
    "2. What happens if you vary eta, the learning rate? \n",
    "3. What is the final value of m_t at the end of the loop? Does that make sense, based on the data generating process?\n",
    "4. Plot the loss vs. iteration "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
