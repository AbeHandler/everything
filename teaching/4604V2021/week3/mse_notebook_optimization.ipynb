{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dmse(xs, ys, m):\n",
    "    '''derivative of mse function (in 1-D)'''\n",
    "    sum_ = 0\n",
    "    for _x, _y in zip(xs, ys):\n",
    "        sum_ += -2 * _x * (_y - (m * _x))\n",
    "    return sum_/len(xs)\n",
    "\n",
    "def mse(preds, ys):\n",
    "    '''\n",
    "    mean squared error function. \n",
    "    preds are predictions, ys are truth\n",
    "    '''\n",
    "    sq = 0 \n",
    "    for p, y in zip(preds, ys):\n",
    "        sq += (p - y) ** 2\n",
    "    return sq/len(ys)\n",
    "\n",
    "def get_preds(m_t, xs):\n",
    "    '''\n",
    "    Get predictions for xs, based on the parameter m_t\n",
    "    '''\n",
    "    preds = []\n",
    "    for _x in xs:\n",
    "        p = m_t * _x\n",
    "        preds.append(p)\n",
    "    return preds\n",
    "\n",
    "def generate_some_data(real_m=2):\n",
    "    '''\n",
    "    Generate some data, using the equation y=mx + e \n",
    "    where e is random Gaussian error\n",
    "    '''\n",
    "    xs = np.linspace(0, 10)\n",
    "    ys = []\n",
    "    for _x in xs:\n",
    "        e = np.random.normal(0, 1, 1)[0]\n",
    "        y = (_x * real_m) + e\n",
    "        ys.append(y)\n",
    "    return xs, ys\n",
    "\n",
    "def plot_line(xs, ys, m_t):\n",
    "    data = []\n",
    "    for x, y in zip(xs, ys):\n",
    "        pred = m_t * x\n",
    "        data.append({\"x\": x, \"y\": y, \"pred\": pred})\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    c = alt.Chart(df).mark_point().encode(\n",
    "        x=\"x\",\n",
    "        y=\"y\"\n",
    "    )\n",
    "\n",
    "    d = alt.Chart(df).mark_line().encode(\n",
    "        x=\"x\",\n",
    "        y=\"pred\"\n",
    "    )\n",
    "\n",
    "    return c + d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning via gradient descent\n",
    "eta = .001 \n",
    "m_t = 1\n",
    "iters = 100\n",
    "\n",
    "xs, ys = generate_some_data()\n",
    "\n",
    "for i in range(iters):\n",
    "    d = dmse(xs, ys, m_t)\n",
    "    m_t -= d * eta\n",
    "    preds = get_preds(m_t, xs)\n",
    "    c = plot_line(xs, ys, m_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions \n",
    "\n",
    "1. What is each line in the cell called \"learning via gradient descent\" doing?\n",
    "2. What happens if you vary eta, the learning rate? \n",
    "3. What is the final value of m_t at the end of the loop? Does that make sense, based on the data generating process?\n",
    "4. Plot the loss vs. iteration \n",
    "5. Implement the random search strategy and plot the loss versus iteration. What do you observe?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
