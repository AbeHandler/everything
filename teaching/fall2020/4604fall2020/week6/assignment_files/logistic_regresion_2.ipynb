{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CU Boulder 4604/5604\n",
    "\n",
    "#### September 28, 2020\n",
    "\n",
    "####  Logistic regression revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression revisited\n",
    "\n",
    "- Last week we learned about logistic regression and regularization\n",
    "- Today, we are going modify the notebook from last week in 2 ways \n",
    "\n",
    "##### New stuff:\n",
    "1. I filled in the missing functions from last week if you are curious. Everyone who submitted last week got full credit. These notebooks are a way to get your hands dirty with real ML code so you learn the materials more deeply. They are not a way for me to quiz you on what you know. That is what HW, quizzes, exams are for ...\n",
    "2. We are going to add a regularization term to the loss function\n",
    "3. We will split training data and test data, and look at the effects of regularization\n",
    "4. We will introduce stochastic gradient descent. The well-known ML researcher [Dave Blei](http://www.cs.columbia.edu/~blei/) says stoachstic gradient descent is like walking from New York to Los Angeles by asking one person at a time for directions. And every person you ask for directions is drunk. Even though this seems like a bad way to get across the country, we will see that is has clear advantages\n",
    "5. I changed the code from gradient ascent to gradient descent (flipping signs as needed). Recall that maximizing log likelihood is the same as minimizing negative log likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annoucements \n",
    "\n",
    "- HW2 is out\n",
    "- This notebook is pretty similar to HW2\n",
    "- In the real world, it is OK to lean on sklearn's implementation of logistic regression\n",
    "- But taking this class gives you a chance to actually understand how it works. This notebook should make the sklearn code much less mysterious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here is the logistic function, the activation function for logistic regression\n",
    "\n",
    "def logistic(z):\n",
    "    return 1/(1 + np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Let's generate some features, weights and y values and compute the loss \n",
    "## This can help you build up intuition for what is happening\n",
    "\n",
    "def log_prob(z, y_i):\n",
    "    '''\n",
    "    Returns the log_prob for one point\n",
    "    '''\n",
    "    fz = logistic(z)\n",
    "    return y_i * np.log(fz) + (1 - y_i) * np.log(1 - fz)\n",
    "\n",
    "\n",
    "out = []\n",
    "\n",
    "dim_ = 10\n",
    "\n",
    "for _ in range(1000):\n",
    "    # generate some random weights \n",
    "    w = np.random.uniform(low=-2, high=2, size=dim_)\n",
    "    \n",
    "    # generate some random binary features \n",
    "    x = (np.random.rand(dim_) > .5).astype(int) \n",
    "    \n",
    "    # get the z score\n",
    "    z = w.dot(x)\n",
    "\n",
    "    # randomly assign y\n",
    "    y = 1 if random.random() < .5 else 0\n",
    "    \n",
    "    # compute the loss\n",
    "    loss = log_prob(z=z, y_i=y)\n",
    "    \n",
    "    # keep track of what is happening\n",
    "    out.append({\"z\": z, \"loss\": loss, \"label\": y})\n",
    "\n",
    "\n",
    "# Plot the results\n",
    "df = pd.DataFrame(out)\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(df[df[\"label\"] == 1][\"z\"], df[df[\"label\"] == 1][\"loss\"], 'x', color=\"red\", label='1')\n",
    "ax.plot(df[df[\"label\"] == 0][\"z\"], df[df[\"label\"] == 0][\"loss\"], 'o', color=\"blue\", label='0')\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"log_prob\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, loss: 5793.811507862259, accuracy: 0.8196\n",
      "iter: 1, loss: 5531.492067116258, accuracy: 0.8196\n",
      "iter: 2, loss: 5217.3737400898735, accuracy: 0.8194\n",
      "iter: 3, loss: 4603.570963825092, accuracy: 0.8212\n",
      "iter: 4, loss: 4229.555574459421, accuracy: 0.8192\n",
      "iter: 5, loss: 3953.6292325296454, accuracy: 0.8212\n",
      "iter: 6, loss: 3959.666941502819, accuracy: 0.823\n",
      "iter: 7, loss: 3713.612427443597, accuracy: 0.8218\n",
      "iter: 8, loss: 3172.2861609198026, accuracy: 0.814\n",
      "iter: 9, loss: 3034.5649577972554, accuracy: 0.8072\n",
      "iter: 10, loss: 2831.6970514607415, accuracy: 0.8054\n",
      "iter: 11, loss: 2628.112617532194, accuracy: 0.8064\n",
      "iter: 12, loss: 2504.702827297553, accuracy: 0.8114\n",
      "iter: 13, loss: 2435.349285711604, accuracy: 0.813\n",
      "iter: 14, loss: 2388.010077070792, accuracy: 0.8124\n",
      "iter: 15, loss: 2283.8172159756323, accuracy: 0.8166\n",
      "iter: 16, loss: 2239.3687537582855, accuracy: 0.8178\n",
      "iter: 17, loss: 2238.3218727254894, accuracy: 0.8272\n",
      "iter: 18, loss: 2131.87020515904, accuracy: 0.831\n",
      "iter: 19, loss: 2126.063827530776, accuracy: 0.8292\n",
      "iter: 20, loss: 2090.450743820381, accuracy: 0.835\n",
      "iter: 21, loss: 2047.8931753410748, accuracy: 0.8352\n",
      "iter: 22, loss: 1942.4617586825734, accuracy: 0.8394\n",
      "iter: 23, loss: 1961.321164063074, accuracy: 0.8364\n",
      "iter: 24, loss: 1903.176878405635, accuracy: 0.8394\n",
      "iter: 25, loss: 1927.4387941677942, accuracy: 0.8364\n",
      "iter: 26, loss: 1893.5026592129796, accuracy: 0.8392\n",
      "iter: 27, loss: 2021.3108073020194, accuracy: 0.833\n",
      "iter: 28, loss: 1980.9787727020016, accuracy: 0.8324\n",
      "iter: 29, loss: 1898.2227325162746, accuracy: 0.8344\n",
      "iter: 30, loss: 1754.303477034027, accuracy: 0.8444\n",
      "iter: 31, loss: 1728.4708617770493, accuracy: 0.8466\n",
      "iter: 32, loss: 1696.2678825287812, accuracy: 0.8522\n",
      "iter: 33, loss: 1698.831803747135, accuracy: 0.85\n",
      "iter: 34, loss: 1701.1733587484164, accuracy: 0.8524\n",
      "iter: 35, loss: 1656.7711228325418, accuracy: 0.853\n",
      "iter: 36, loss: 1636.9332599559648, accuracy: 0.8542\n",
      "iter: 37, loss: 1626.5410998330633, accuracy: 0.8494\n",
      "iter: 38, loss: 1607.051182282106, accuracy: 0.85\n",
      "iter: 39, loss: 1616.7624499367132, accuracy: 0.8432\n",
      "iter: 40, loss: 1624.2170804642765, accuracy: 0.838\n",
      "iter: 41, loss: 1574.9185871460309, accuracy: 0.8584\n",
      "iter: 42, loss: 1551.1311245311924, accuracy: 0.846\n",
      "iter: 43, loss: 1535.867331093959, accuracy: 0.8576\n",
      "iter: 44, loss: 1522.5218164342434, accuracy: 0.8438\n",
      "iter: 45, loss: 1503.3046839214237, accuracy: 0.8442\n",
      "iter: 46, loss: 1495.0380612372282, accuracy: 0.8436\n",
      "iter: 47, loss: 1497.36859074249, accuracy: 0.8404\n",
      "iter: 48, loss: 1493.0764976649245, accuracy: 0.8422\n",
      "iter: 49, loss: 1495.8865376977665, accuracy: 0.851\n",
      "iter: 50, loss: 1484.866676963168, accuracy: 0.8444\n",
      "iter: 51, loss: 1497.7053056315997, accuracy: 0.8378\n",
      "iter: 52, loss: 1539.5635094459687, accuracy: 0.8312\n",
      "iter: 53, loss: 1513.9317764664336, accuracy: 0.8342\n",
      "iter: 54, loss: 1515.906001699209, accuracy: 0.8372\n",
      "iter: 55, loss: 1515.2727690229651, accuracy: 0.84\n",
      "iter: 56, loss: 1531.9242011940978, accuracy: 0.8516\n",
      "iter: 57, loss: 1632.140193966229, accuracy: 0.8684\n",
      "iter: 58, loss: 1521.0692694945717, accuracy: 0.8484\n",
      "iter: 59, loss: 1581.1078825372188, accuracy: 0.865\n",
      "iter: 60, loss: 1620.0655429665821, accuracy: 0.8674\n",
      "iter: 61, loss: 1580.0193188899082, accuracy: 0.8638\n",
      "iter: 62, loss: 1512.0691603332612, accuracy: 0.852\n",
      "iter: 63, loss: 1490.8659212676246, accuracy: 0.842\n",
      "iter: 64, loss: 1501.175775804583, accuracy: 0.8336\n",
      "iter: 65, loss: 1687.07961797316, accuracy: 0.8784\n",
      "iter: 66, loss: 1696.0952667256536, accuracy: 0.8798\n",
      "iter: 67, loss: 1528.0316520607719, accuracy: 0.8554\n",
      "iter: 68, loss: 1517.577569614468, accuracy: 0.8502\n",
      "iter: 69, loss: 1521.9430030832555, accuracy: 0.8502\n",
      "iter: 70, loss: 1536.6484153156866, accuracy: 0.8514\n",
      "iter: 71, loss: 1536.830540855191, accuracy: 0.8446\n",
      "iter: 72, loss: 1608.45364248483, accuracy: 0.8624\n",
      "iter: 73, loss: 1511.4546909218616, accuracy: 0.8388\n",
      "iter: 74, loss: 1509.3500376959846, accuracy: 0.8454\n",
      "iter: 75, loss: 1514.7978393660178, accuracy: 0.847\n",
      "iter: 76, loss: 1613.2916229976404, accuracy: 0.8722\n",
      "iter: 77, loss: 1499.0874942128219, accuracy: 0.8462\n",
      "iter: 78, loss: 1567.7573893061403, accuracy: 0.8642\n",
      "iter: 79, loss: 1502.8125214016477, accuracy: 0.8474\n",
      "iter: 80, loss: 1569.9130464645118, accuracy: 0.8648\n",
      "iter: 81, loss: 1601.8103603393915, accuracy: 0.8688\n",
      "iter: 82, loss: 1495.6062494367807, accuracy: 0.8442\n",
      "iter: 83, loss: 1874.654041786439, accuracy: 0.8806\n",
      "iter: 84, loss: 1591.7372581818643, accuracy: 0.8674\n",
      "iter: 85, loss: 1491.2318054327197, accuracy: 0.8404\n",
      "iter: 86, loss: 1485.1090486178923, accuracy: 0.8406\n",
      "iter: 87, loss: 1486.163401578486, accuracy: 0.8444\n",
      "iter: 88, loss: 1504.6973357460176, accuracy: 0.8374\n",
      "iter: 89, loss: 1516.4967292794563, accuracy: 0.8342\n",
      "iter: 90, loss: 1508.7081854772964, accuracy: 0.8342\n",
      "iter: 91, loss: 1510.5974055678143, accuracy: 0.8472\n",
      "iter: 92, loss: 1578.5386245736588, accuracy: 0.8636\n",
      "iter: 93, loss: 1501.0250357480668, accuracy: 0.846\n",
      "iter: 94, loss: 1524.4788617631873, accuracy: 0.8484\n",
      "iter: 95, loss: 1530.8192223358428, accuracy: 0.8474\n",
      "iter: 96, loss: 1530.7582483235385, accuracy: 0.8474\n",
      "iter: 97, loss: 1528.539207461264, accuracy: 0.8496\n",
      "iter: 98, loss: 1506.1578765462884, accuracy: 0.8376\n",
      "iter: 99, loss: 1504.5365064181385, accuracy: 0.838\n",
      "iter: 100, loss: 1504.0029427619847, accuracy: 0.8382\n",
      "0.1 2.4151078130236514 0.836\n"
     ]
    }
   ],
   "source": [
    "def neg_log_likelihood(X, w, y):\n",
    "    '''Compute the negative log likelihood'''\n",
    "    L = 0\n",
    "    for _x,_y in zip(X, y):\n",
    "        z = w.dot(_x)\n",
    "        L += log_prob(z=z, y_i=_y)\n",
    "    return -1 * L\n",
    "\n",
    "\n",
    "def fast_logistic(X, w):\n",
    "    '''Compute the logistic function over many data points'''\n",
    "    return 1/(1 + np.exp(-1 * X.dot(w)))\n",
    "\n",
    "\n",
    "def grad(_X, w, _y, lambda_=.5):\n",
    "    '''\n",
    "    Return the gradient\n",
    "    \n",
    "    - https://web.stanford.edu/~jurafsky/slp3/5.pdf\n",
    "    '''\n",
    "    grad = np.zeros_like(w)\n",
    "    \n",
    "    N,D= _X.shape\n",
    "    \n",
    "    b = _X * (fast_logistic(_X, w) - _y).reshape((N, 1))\n",
    "\n",
    "    return np.sum(b, axis=0) + (lambda_ * 2 * w)\n",
    "\n",
    "\n",
    "def squared_l2_norm(x):\n",
    "    '''\n",
    "    $\\sqrt{\\Sigma x_i^2} ^ 2\n",
    "    '''\n",
    "    return np.sum(x ** 2)\n",
    "\n",
    "\n",
    "def grad_decent(_X, _y, eta = .0001, lambda_ = 0, tolerance=1e-4, verbose=True, batch_size=None, iters=None):\n",
    "    '''\n",
    "    - Perform gradient ascent\n",
    "    - This function is basically the same as in the Adeline notebook\n",
    "    - Of course, the gradient is different, because it is a different function\n",
    "    '''\n",
    "    w = np.random.uniform(low=-5, high=2, size=dim_)\n",
    "    last = 0\n",
    "    losses = []\n",
    "    for i in range(1000):\n",
    "        if i > iters and iters is not None:\n",
    "            break\n",
    "        this_ll = neg_log_likelihood(_X, w, _y)\n",
    "        loss = this_ll + lambda_ * squared_l2_norm(w)\n",
    "        losses.append(loss)\n",
    "        if verbose:\n",
    "            print(\"iter: {}, loss: {}, accuracy: {}\".format(i, loss, accuracy(_X, w, _y)))\n",
    "        if(abs(this_ll - last) < tolerance): break\n",
    "        last = this_ll\n",
    "        \n",
    "        if batch_size is None:\n",
    "            w -= eta * grad(_X, w, _y, lambda_=lambda_)\n",
    "        else:\n",
    "            _N,F = _X.shape\n",
    "            idx = np.random.randint(_N, size=batch_size)\n",
    "            w -= eta * grad(_X[idx], w, _y[idx], lambda_=lambda_)/batch_size\n",
    "        \n",
    "    return w, losses\n",
    "\n",
    "def prediction(X, w, threshold=.5):\n",
    "    '''\n",
    "    - Return a Boolean array of length N.\n",
    "    - The array should be True if the weights dotted with the features for a given instance is greater than .5\n",
    "    '''\n",
    "    N, D = X.shape\n",
    "    return X.dot(w) > threshold\n",
    "\n",
    "def accuracy(X, w, y):\n",
    "    '''\n",
    "    Return a value between 0 and 1, showing the fraction of data points which have been classified correctly\n",
    "    '''\n",
    "    return np.mean(prediction(X, w) == y)\n",
    "\n",
    "def init_data(N, dim_):\n",
    "    '''\n",
    "    Initialize data. Note how we generate y below. We know how the data is generated.\n",
    "    '''\n",
    "    w = np.random.uniform(low=-1, high=1, size=dim_)\n",
    "    X = (np.random.rand(dim_ * N) > .5).astype(int)\n",
    "    X = X.reshape(N, dim_)\n",
    "\n",
    "    z_ = X.dot(w) + np.random.uniform(low=-1, high=1, size=X.dot(w).size)\n",
    "\n",
    "    y =  1/(1 + np.exp(-1 * z_)) > .5\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "N = 10000\n",
    "dim_ = 10\n",
    "\n",
    "w = np.random.uniform(low=-5, high=2, size=dim_)\n",
    "\n",
    "X, y = init_data(N, dim_)\n",
    "\n",
    "split = int(N/2)\n",
    "\n",
    "X_train = X[0:split]\n",
    "X_test = X[split:]\n",
    "y_train = y[0:split]\n",
    "y_test = y[split:]\n",
    "\n",
    "for lambda_ in [.1]: #[0, .01, .1, .25, 100, 1000, 5000]:\n",
    "    w, losses = grad_decent(X_train, y_train, eta=1, tolerance=.0001, iters=100, verbose=True, lambda_=lambda_, batch_size=10)\n",
    "    print(lambda_, np.linalg.norm(w), accuracy(X_train, w, y_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "- Complete the L2 norm function\n",
    "- What does the variable `lambda` do in the code above? \n",
    "- What happens if you set to a huge number? What happens if you set it to a small number?\n",
    "- Try computing the norm of w with different lambda\n",
    "- How noisy is the optimization if you vary the loss\n",
    "- Print the loss and vary the batch size\n",
    "- How do you think that varying eta will vary the noise \n",
    "- How do you think that varying batch size will vary noise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
